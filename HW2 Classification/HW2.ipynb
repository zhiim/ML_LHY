{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UniKqPi-Cps",
        "outputId": "be50a3f2-e2a3-42d5-e1f3-6b429c90ce8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.4\n",
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qzCRnywKh30mTbWUEjXuNT2isOCAPdO1\n",
            "To: /content/libriphone.zip\n",
            "100% 384M/384M [00:03<00:00, 128MB/s]\n",
            "feat  test_split.txt  train_labels.txt\ttrain_split.txt\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gdown\n",
        "\n",
        "# Main link\n",
        "# !gdown --id '1N1eVIDe9hKM5uiNRGmifBlwSDGiVXPJe' --output libriphone.zip\n",
        "!gdown --id '1qzCRnywKh30mTbWUEjXuNT2isOCAPdO1' --output libriphone.zip\n",
        "\n",
        "!unzip -q libriphone.zip\n",
        "!ls libriphone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!mkdir models\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnHtmN1msdBF",
        "outputId": "5634904d-4a9c-416c-eed4-40fe86075a19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libriphone  libriphone.zip  sample_data\n",
            "libriphone  libriphone.zip  models  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_feat(path):\n",
        "    feat = torch.load(path)\n",
        "    return feat\n",
        "\n",
        "def shift(x, n):\n",
        "    if n < 0:\n",
        "        left = x[0].repeat(-n, 1)\n",
        "        right = x[:n]\n",
        "    elif n > 0:\n",
        "        right = x[-1].repeat(n, 1)\n",
        "        left = x[n:]\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "    return torch.cat((left, right), dim=0)\n",
        "\n",
        "def concat_feat(x, concat_n):\n",
        "    assert concat_n % 2 == 1 # n must be odd\n",
        "    if concat_n < 2:\n",
        "        return x\n",
        "    seq_len, feature_dim = x.size(0), x.size(1)\n",
        "    x = x.repeat(1, concat_n) \n",
        "    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) # concat_n, seq_len, feature_dim\n",
        "    mid = (concat_n // 2)\n",
        "    for r_idx in range(1, mid+1):\n",
        "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
        "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
        "\n",
        "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n",
        "\n",
        "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8, random_seed=1213):\n",
        "    class_num = 41 # NOTE: pre-computed, should not need change\n",
        "\n",
        "    if split == 'train' or split == 'val':\n",
        "        mode = 'train'\n",
        "    elif split == 'test':\n",
        "        mode = 'test'\n",
        "    else:\n",
        "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
        "\n",
        "    label_dict = {}\n",
        "    if mode == 'train':\n",
        "        for line in open(os.path.join(phone_path, f'{mode}_labels.txt')).readlines():\n",
        "            line = line.strip('\\n').split(' ')\n",
        "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
        "        \n",
        "        # split training and validation data\n",
        "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(usage_list)\n",
        "        train_len = int(len(usage_list) * train_ratio)\n",
        "        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n",
        "\n",
        "    elif mode == 'test':\n",
        "        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
        "\n",
        "    usage_list = [line.strip('\\n') for line in usage_list]\n",
        "    print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n",
        "\n",
        "    max_len = 3000000\n",
        "    X = torch.empty(max_len, 39 * concat_nframes)\n",
        "    if mode == 'train':\n",
        "        y = torch.empty(max_len, dtype=torch.long)\n",
        "\n",
        "    idx = 0\n",
        "    for i, fname in tqdm(enumerate(usage_list)):\n",
        "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
        "        cur_len = len(feat)\n",
        "        feat = concat_feat(feat, concat_nframes)\n",
        "        if mode == 'train':\n",
        "          label = torch.LongTensor(label_dict[fname])\n",
        "\n",
        "        X[idx: idx + cur_len, :] = feat\n",
        "        if mode == 'train':\n",
        "          y[idx: idx + cur_len] = label\n",
        "\n",
        "        idx += cur_len\n",
        "\n",
        "    X = X[:idx, :]\n",
        "    if mode == 'train':\n",
        "      y = y[:idx]\n",
        "\n",
        "    print(f'[INFO] {split} set')\n",
        "    print(X.shape)\n",
        "    if mode == 'train':\n",
        "      print(y.shape)\n",
        "      return X, y\n",
        "    else:\n",
        "      return X"
      ],
      "metadata": {
        "id": "SCPuZSEjXXKY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def same_seeds(seed):\n",
        "    random.seed(seed) \n",
        "    np.random.seed(seed)  \n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed) \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "T8XHpBBZZATq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data prarameters\n",
        "# TODO: change the value of \"concat_nframes\" for medium baseline\n",
        "concat_nframes = 3   # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n",
        "train_ratio = 0.75   # the ratio of data used for training, the rest will be used for validation\n",
        "\n",
        "# training parameters\n",
        "seed = 1213          # random seed"
      ],
      "metadata": {
        "id": "wPG2LqeHXmu0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes for dataset, network\n",
        "# and several fuctions for data processing and trainnig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# support gpu or not\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# split data into training data and validation data\n",
        "def split_data(data, label):\n",
        "    VAL_RATIO = 0.2\n",
        "\n",
        "    percent = int(data.shape[0] * (1 - VAL_RATIO))\n",
        "    train_data, train_label, val_data, val_label = data[:percent], label[:percent], data[percent:], label[percent:]\n",
        "    print('Size of training set: {}'.format(train_data.shape))\n",
        "    print('Size of validation set: {}'.format(val_data.shape))\n",
        "    return train_data, train_label, val_data, val_label\n",
        "\n",
        "\n",
        "# a custom Dataset, load data from .npy files\n",
        "class VoiceDataset(Dataset):\n",
        "    def __init__(self, data_train, data_label=None):\n",
        "        super().__init__()\n",
        "        self.data_train = data_train  # data for training\n",
        "        if data_label is not None:\n",
        "            self.data_label = torch.LongTensor(data_label)  # label for data used for training\n",
        "        else:\n",
        "            self.data_label = None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.data_label is not None:\n",
        "            return self.data_train[index], self.data_label[index]  # return a element in dataset according to index\n",
        "        else:\n",
        "            return self.data_train[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_train)  # return the length of this dataset\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size) -> None:\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # define the network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 1024),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # return the output of network\n",
        "        return self.net(input)\n",
        "    \n",
        "    def cal_loss(self, pred, target):\n",
        "        self.criterion = nn.CrossEntropyLoss()  # set loss function to Cross Entropy\n",
        "        return self.criterion(pred, target)\n",
        "\n",
        "\n",
        "# this function includes everything for training\n",
        "def train(tr_set, dv_set, model, config, device):\n",
        "    num_epochs = config['num_epochs']  # number of epochs\n",
        "\n",
        "    # set the optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "    \n",
        "    # init parameters for epochs\n",
        "    max_acc = 0.  # set the initial value for min_mse (higher than the mse after first epoch)\n",
        "    loss_record = {'train': [], 'val': []}\n",
        "    acc_record = {'train': [], 'val': []}\n",
        "    \n",
        "    # epochs for trianing\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc = 0\n",
        "        train_loss = 0\n",
        "\n",
        "        model.train()  # set model to trian mode\n",
        "        for i, (data, label) in enumerate(tr_set):\n",
        "            optimizer.zero_grad()  # set gradient to zero before calculate\n",
        "            data, label = data.to(device), label.to(device)  # move data to device\n",
        "            pred = model(data)  # compute the predict from data\n",
        "            loss = model.cal_loss(pred, label)  # compute the mse loss\n",
        "            _, train_pred = torch.max(pred, 1) # get the index of the class with the highest probability\n",
        "            loss.backward()  # get the gradient\n",
        "            optimizer.step()  # updata parameters in model\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_acc += (train_pred.cpu() == label.cpu()).sum().item()\n",
        "\n",
        "            # get loss as a tensor in cpu (item() cannot access tensor in cuda) \n",
        "            # without gradient computing requirement and trun it to a variant\n",
        "            loss_record['train'].append(loss.detach().cpu().item())\n",
        "\n",
        "            # print training status every 100 optimisations\n",
        "            if (i+1) % 500 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, len(tr_set), loss.item()))\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        val_acc, val_loss = val(dv_set, model, device)\n",
        "\n",
        "        train_acc = train_acc/len(tr_set.dataset)\n",
        "        train_loss = train_loss/len(tr_set)\n",
        "\n",
        "        acc_record['train'].append(train_acc)\n",
        "\n",
        "        print('[{:03d}/{:03d}] Train ACC: {:3.6f} Loss: {:3.6f} | Val ACC: {:3.6f} loss: {:3.6f}'.format(epoch+1, num_epochs, train_acc, train_loss, val_acc, val_loss))\n",
        "\n",
        "        if val_acc > max_acc:\n",
        "            # Save model if your model improved\n",
        "            max_acc = val_acc\n",
        "            print('Saving model (epoch = {:4d}, acc = {:.4f})'\n",
        "                .format(epoch + 1, max_acc))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        loss_record['val'].append(val_loss)\n",
        "        acc_record['val'].append(val_acc)\n",
        "\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "    \n",
        "    print('Finished training after {} epochs'.format(epoch+1))\n",
        "    return acc_record, loss_record, max_acc\n",
        "\n",
        "\n",
        "# fuction to compute mse in validation\n",
        "def val(dv_set, model, device):\n",
        "    model.eval()  # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    val_acc = 0\n",
        "    for x, y in dv_set:  # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)  # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():  # disable gradient calculation\n",
        "            pred = model(x)  # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "\n",
        "        total_loss += mse_loss.detach().cpu().item()  # accumulate loss\n",
        "\n",
        "        _, val_pred = torch.max(pred, 1)\n",
        "        val_acc += (val_pred.cpu() == y.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "\n",
        "    return val_acc/len(dv_set.dataset), total_loss / len(dv_set)\n",
        "\n",
        "\n",
        "# function for model test\n",
        "def test(tt_set, model, device):\n",
        "    model.eval()  # set model to evalutation mode\n",
        "    preds = []\n",
        "\n",
        "    for data in tt_set:\n",
        "        data = data.to(device)\n",
        "        # we don't need compute gradient when testing\n",
        "        with torch.no_grad():\n",
        "            pred = model(data)\n",
        "            _, test_pred = torch.max(pred, 1) # get the index of the class with the highest probability\n",
        "            # dataloader process the dataset as batch\n",
        "            for y in test_pred.cpu().numpy():\n",
        "                preds.append(y)\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "rXcWSFXsqCaa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "\n",
        "same_seeds(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# preprocess data\n",
        "train_X, train_y = preprocess_data(split='train', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n",
        "val_X, val_y = preprocess_data(split='val', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Y_LAfrJSSr",
        "outputId": "884443de-4d10-4451-9c4b-a4eb1e2a96af"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n",
            "[Dataset] - # phone classes: 41, number of utterances for train: 2571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2571it [00:01, 1376.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train set\n",
            "torch.Size([1588590, 117])\n",
            "torch.Size([1588590])\n",
            "[Dataset] - # phone classes: 41, number of utterances for val: 858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "858it [00:00, 1169.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] val set\n",
            "torch.Size([528204, 117])\n",
            "torch.Size([528204])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMbAxNoytFhv",
        "outputId": "a5765a25-297c-4c71-8c85-bc68908378e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libriphone  libriphone.zip  models  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network for voice frame classification\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "# parameters for training\n",
        "config = {\n",
        "    'num_epochs': 100,\n",
        "    'batch_size': 512,\n",
        "    'optimizer': 'Adam',  # optimizer algorithm\n",
        "    'optim_hparas': {\n",
        "        'lr': 0.0001  # learning rate\n",
        "    },\n",
        "    'save_path': 'models/model.pth',\n",
        "    'early_stop': 20\n",
        "}\n",
        "\n",
        "train_set = VoiceDataset(train_X, train_y)  # dataset for training\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "val_set = VoiceDataset(val_X, val_y)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# delete the data loaded to save space\n",
        "del train_X, train_y, val_X, val_y\n",
        "gc.collect()\n",
        "\n",
        "model = NeuralNet(39 * concat_nframes, 41).to(device)  # create network\n",
        "\n",
        "acc_record, loss_record, max_acc = train(train_loader, val_loader, model, config, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOAxJW3NqbN0",
        "outputId": "4d1fb70a-620b-4008-96c6-ceca3a32e654"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [500/3103], Loss: 2.7834\n",
            "Epoch [1/100], Step [1000/3103], Loss: 2.4466\n",
            "Epoch [1/100], Step [1500/3103], Loss: 2.2453\n",
            "Epoch [1/100], Step [2000/3103], Loss: 2.1414\n",
            "Epoch [1/100], Step [2500/3103], Loss: 2.0637\n",
            "Epoch [1/100], Step [3000/3103], Loss: 2.1165\n",
            "[001/100] Train ACC: 0.349360 Loss: 2.391172 | Val ACC: 0.420991 loss: 2.038637\n",
            "Saving model (epoch =    1, acc = 0.4210)\n",
            "Epoch [2/100], Step [500/3103], Loss: 2.0385\n",
            "Epoch [2/100], Step [1000/3103], Loss: 2.0893\n",
            "Epoch [2/100], Step [1500/3103], Loss: 1.9131\n",
            "Epoch [2/100], Step [2000/3103], Loss: 1.9462\n",
            "Epoch [2/100], Step [2500/3103], Loss: 2.0184\n",
            "Epoch [2/100], Step [3000/3103], Loss: 2.0198\n",
            "[002/100] Train ACC: 0.440539 Loss: 1.961387 | Val ACC: 0.449080 loss: 1.919846\n",
            "Saving model (epoch =    2, acc = 0.4491)\n",
            "Epoch [3/100], Step [500/3103], Loss: 1.8348\n",
            "Epoch [3/100], Step [1000/3103], Loss: 1.8831\n",
            "Epoch [3/100], Step [1500/3103], Loss: 1.8166\n",
            "Epoch [3/100], Step [2000/3103], Loss: 1.8725\n",
            "Epoch [3/100], Step [2500/3103], Loss: 1.8630\n",
            "Epoch [3/100], Step [3000/3103], Loss: 1.8950\n",
            "[003/100] Train ACC: 0.460364 Loss: 1.873341 | Val ACC: 0.464777 loss: 1.850391\n",
            "Saving model (epoch =    3, acc = 0.4648)\n",
            "Epoch [4/100], Step [500/3103], Loss: 1.7718\n",
            "Epoch [4/100], Step [1000/3103], Loss: 1.6950\n",
            "Epoch [4/100], Step [1500/3103], Loss: 1.8275\n",
            "Epoch [4/100], Step [2000/3103], Loss: 1.7965\n",
            "Epoch [4/100], Step [2500/3103], Loss: 1.8478\n",
            "Epoch [4/100], Step [3000/3103], Loss: 1.7610\n",
            "[004/100] Train ACC: 0.473815 Loss: 1.814318 | Val ACC: 0.476426 loss: 1.800822\n",
            "Saving model (epoch =    4, acc = 0.4764)\n",
            "Epoch [5/100], Step [500/3103], Loss: 1.6832\n",
            "Epoch [5/100], Step [1000/3103], Loss: 1.7091\n",
            "Epoch [5/100], Step [1500/3103], Loss: 1.7057\n",
            "Epoch [5/100], Step [2000/3103], Loss: 1.7172\n",
            "Epoch [5/100], Step [2500/3103], Loss: 1.7443\n",
            "Epoch [5/100], Step [3000/3103], Loss: 1.8206\n",
            "[005/100] Train ACC: 0.484422 Loss: 1.769403 | Val ACC: 0.485027 loss: 1.763746\n",
            "Saving model (epoch =    5, acc = 0.4850)\n",
            "Epoch [6/100], Step [500/3103], Loss: 1.7144\n",
            "Epoch [6/100], Step [1000/3103], Loss: 1.8447\n",
            "Epoch [6/100], Step [1500/3103], Loss: 1.7670\n",
            "Epoch [6/100], Step [2000/3103], Loss: 1.7328\n",
            "Epoch [6/100], Step [2500/3103], Loss: 1.7583\n",
            "Epoch [6/100], Step [3000/3103], Loss: 1.6291\n",
            "[006/100] Train ACC: 0.493953 Loss: 1.733268 | Val ACC: 0.494421 loss: 1.732057\n",
            "Saving model (epoch =    6, acc = 0.4944)\n",
            "Epoch [7/100], Step [500/3103], Loss: 1.6536\n",
            "Epoch [7/100], Step [1000/3103], Loss: 1.7332\n",
            "Epoch [7/100], Step [1500/3103], Loss: 1.7377\n",
            "Epoch [7/100], Step [2000/3103], Loss: 1.7340\n",
            "Epoch [7/100], Step [2500/3103], Loss: 1.8001\n",
            "Epoch [7/100], Step [3000/3103], Loss: 1.5948\n",
            "[007/100] Train ACC: 0.501120 Loss: 1.703031 | Val ACC: 0.500492 loss: 1.702503\n",
            "Saving model (epoch =    7, acc = 0.5005)\n",
            "Epoch [8/100], Step [500/3103], Loss: 1.8414\n",
            "Epoch [8/100], Step [1000/3103], Loss: 1.7313\n",
            "Epoch [8/100], Step [1500/3103], Loss: 1.6981\n",
            "Epoch [8/100], Step [2000/3103], Loss: 1.6441\n",
            "Epoch [8/100], Step [2500/3103], Loss: 1.7086\n",
            "Epoch [8/100], Step [3000/3103], Loss: 1.6957\n",
            "[008/100] Train ACC: 0.507574 Loss: 1.677775 | Val ACC: 0.505411 loss: 1.682667\n",
            "Saving model (epoch =    8, acc = 0.5054)\n",
            "Epoch [9/100], Step [500/3103], Loss: 1.5932\n",
            "Epoch [9/100], Step [1000/3103], Loss: 1.6748\n",
            "Epoch [9/100], Step [1500/3103], Loss: 1.6614\n",
            "Epoch [9/100], Step [2000/3103], Loss: 1.7238\n",
            "Epoch [9/100], Step [2500/3103], Loss: 1.6140\n",
            "Epoch [9/100], Step [3000/3103], Loss: 1.7624\n",
            "[009/100] Train ACC: 0.512731 Loss: 1.655678 | Val ACC: 0.510040 loss: 1.660790\n",
            "Saving model (epoch =    9, acc = 0.5100)\n",
            "Epoch [10/100], Step [500/3103], Loss: 1.5637\n",
            "Epoch [10/100], Step [1000/3103], Loss: 1.6706\n",
            "Epoch [10/100], Step [1500/3103], Loss: 1.5944\n",
            "Epoch [10/100], Step [2000/3103], Loss: 1.6751\n",
            "Epoch [10/100], Step [2500/3103], Loss: 1.4750\n",
            "Epoch [10/100], Step [3000/3103], Loss: 1.5454\n",
            "[010/100] Train ACC: 0.517284 Loss: 1.636266 | Val ACC: 0.513275 loss: 1.647355\n",
            "Saving model (epoch =   10, acc = 0.5133)\n",
            "Epoch [11/100], Step [500/3103], Loss: 1.5147\n",
            "Epoch [11/100], Step [1000/3103], Loss: 1.5322\n",
            "Epoch [11/100], Step [1500/3103], Loss: 1.4997\n",
            "Epoch [11/100], Step [2000/3103], Loss: 1.6248\n",
            "Epoch [11/100], Step [2500/3103], Loss: 1.6380\n",
            "Epoch [11/100], Step [3000/3103], Loss: 1.6143\n",
            "[011/100] Train ACC: 0.521257 Loss: 1.618919 | Val ACC: 0.517440 loss: 1.630474\n",
            "Saving model (epoch =   11, acc = 0.5174)\n",
            "Epoch [12/100], Step [500/3103], Loss: 1.5474\n",
            "Epoch [12/100], Step [1000/3103], Loss: 1.5688\n",
            "Epoch [12/100], Step [1500/3103], Loss: 1.6188\n",
            "Epoch [12/100], Step [2000/3103], Loss: 1.5954\n",
            "Epoch [12/100], Step [2500/3103], Loss: 1.5258\n",
            "Epoch [12/100], Step [3000/3103], Loss: 1.6440\n",
            "[012/100] Train ACC: 0.524943 Loss: 1.603370 | Val ACC: 0.520670 loss: 1.615487\n",
            "Saving model (epoch =   12, acc = 0.5207)\n",
            "Epoch [13/100], Step [500/3103], Loss: 1.6170\n",
            "Epoch [13/100], Step [1000/3103], Loss: 1.5982\n",
            "Epoch [13/100], Step [1500/3103], Loss: 1.5727\n",
            "Epoch [13/100], Step [2000/3103], Loss: 1.5781\n",
            "Epoch [13/100], Step [2500/3103], Loss: 1.5282\n",
            "Epoch [13/100], Step [3000/3103], Loss: 1.6589\n",
            "[013/100] Train ACC: 0.528283 Loss: 1.589764 | Val ACC: 0.523436 loss: 1.604224\n",
            "Saving model (epoch =   13, acc = 0.5234)\n",
            "Epoch [14/100], Step [500/3103], Loss: 1.5303\n",
            "Epoch [14/100], Step [1000/3103], Loss: 1.6030\n",
            "Epoch [14/100], Step [1500/3103], Loss: 1.6428\n",
            "Epoch [14/100], Step [2000/3103], Loss: 1.5906\n",
            "Epoch [14/100], Step [2500/3103], Loss: 1.5990\n",
            "Epoch [14/100], Step [3000/3103], Loss: 1.6352\n",
            "[014/100] Train ACC: 0.531221 Loss: 1.577100 | Val ACC: 0.524945 loss: 1.596854\n",
            "Saving model (epoch =   14, acc = 0.5249)\n",
            "Epoch [15/100], Step [500/3103], Loss: 1.5913\n",
            "Epoch [15/100], Step [1000/3103], Loss: 1.5689\n",
            "Epoch [15/100], Step [1500/3103], Loss: 1.6196\n",
            "Epoch [15/100], Step [2000/3103], Loss: 1.5904\n",
            "Epoch [15/100], Step [2500/3103], Loss: 1.5548\n",
            "Epoch [15/100], Step [3000/3103], Loss: 1.4658\n",
            "[015/100] Train ACC: 0.533927 Loss: 1.565974 | Val ACC: 0.527116 loss: 1.587185\n",
            "Saving model (epoch =   15, acc = 0.5271)\n",
            "Epoch [16/100], Step [500/3103], Loss: 1.5703\n",
            "Epoch [16/100], Step [1000/3103], Loss: 1.5342\n",
            "Epoch [16/100], Step [1500/3103], Loss: 1.5005\n",
            "Epoch [16/100], Step [2000/3103], Loss: 1.6003\n",
            "Epoch [16/100], Step [2500/3103], Loss: 1.6321\n",
            "Epoch [16/100], Step [3000/3103], Loss: 1.5317\n",
            "[016/100] Train ACC: 0.536638 Loss: 1.555950 | Val ACC: 0.529123 loss: 1.582229\n",
            "Saving model (epoch =   16, acc = 0.5291)\n",
            "Epoch [17/100], Step [500/3103], Loss: 1.6175\n",
            "Epoch [17/100], Step [1000/3103], Loss: 1.5311\n",
            "Epoch [17/100], Step [1500/3103], Loss: 1.5043\n",
            "Epoch [17/100], Step [2000/3103], Loss: 1.4111\n",
            "Epoch [17/100], Step [2500/3103], Loss: 1.5536\n",
            "Epoch [17/100], Step [3000/3103], Loss: 1.4951\n",
            "[017/100] Train ACC: 0.538661 Loss: 1.546686 | Val ACC: 0.531789 loss: 1.571588\n",
            "Saving model (epoch =   17, acc = 0.5318)\n",
            "Epoch [18/100], Step [500/3103], Loss: 1.7119\n",
            "Epoch [18/100], Step [1000/3103], Loss: 1.5845\n",
            "Epoch [18/100], Step [1500/3103], Loss: 1.5969\n",
            "Epoch [18/100], Step [2000/3103], Loss: 1.5825\n",
            "Epoch [18/100], Step [2500/3103], Loss: 1.4236\n",
            "Epoch [18/100], Step [3000/3103], Loss: 1.5483\n",
            "[018/100] Train ACC: 0.540842 Loss: 1.538200 | Val ACC: 0.532715 loss: 1.565029\n",
            "Saving model (epoch =   18, acc = 0.5327)\n",
            "Epoch [19/100], Step [500/3103], Loss: 1.4798\n",
            "Epoch [19/100], Step [1000/3103], Loss: 1.5588\n",
            "Epoch [19/100], Step [1500/3103], Loss: 1.6601\n",
            "Epoch [19/100], Step [2000/3103], Loss: 1.5817\n",
            "Epoch [19/100], Step [2500/3103], Loss: 1.4716\n",
            "Epoch [19/100], Step [3000/3103], Loss: 1.4051\n",
            "[019/100] Train ACC: 0.542886 Loss: 1.530414 | Val ACC: 0.534379 loss: 1.559989\n",
            "Saving model (epoch =   19, acc = 0.5344)\n",
            "Epoch [20/100], Step [500/3103], Loss: 1.3633\n",
            "Epoch [20/100], Step [1000/3103], Loss: 1.4316\n",
            "Epoch [20/100], Step [1500/3103], Loss: 1.6036\n",
            "Epoch [20/100], Step [2000/3103], Loss: 1.5187\n",
            "Epoch [20/100], Step [2500/3103], Loss: 1.5444\n",
            "Epoch [20/100], Step [3000/3103], Loss: 1.5730\n",
            "[020/100] Train ACC: 0.544682 Loss: 1.523116 | Val ACC: 0.535960 loss: 1.553777\n",
            "Saving model (epoch =   20, acc = 0.5360)\n",
            "Epoch [21/100], Step [500/3103], Loss: 1.5195\n",
            "Epoch [21/100], Step [1000/3103], Loss: 1.6463\n",
            "Epoch [21/100], Step [1500/3103], Loss: 1.5788\n",
            "Epoch [21/100], Step [2000/3103], Loss: 1.6211\n",
            "Epoch [21/100], Step [2500/3103], Loss: 1.5889\n",
            "Epoch [21/100], Step [3000/3103], Loss: 1.6011\n",
            "[021/100] Train ACC: 0.546337 Loss: 1.516202 | Val ACC: 0.537518 loss: 1.547170\n",
            "Saving model (epoch =   21, acc = 0.5375)\n",
            "Epoch [22/100], Step [500/3103], Loss: 1.5122\n",
            "Epoch [22/100], Step [1000/3103], Loss: 1.4691\n",
            "Epoch [22/100], Step [1500/3103], Loss: 1.4833\n",
            "Epoch [22/100], Step [2000/3103], Loss: 1.5442\n",
            "Epoch [22/100], Step [2500/3103], Loss: 1.5159\n",
            "Epoch [22/100], Step [3000/3103], Loss: 1.5515\n",
            "[022/100] Train ACC: 0.547965 Loss: 1.509861 | Val ACC: 0.537769 loss: 1.544608\n",
            "Saving model (epoch =   22, acc = 0.5378)\n",
            "Epoch [23/100], Step [500/3103], Loss: 1.5282\n",
            "Epoch [23/100], Step [1000/3103], Loss: 1.5180\n",
            "Epoch [23/100], Step [1500/3103], Loss: 1.5015\n",
            "Epoch [23/100], Step [2000/3103], Loss: 1.4954\n",
            "Epoch [23/100], Step [2500/3103], Loss: 1.5392\n",
            "Epoch [23/100], Step [3000/3103], Loss: 1.4812\n",
            "[023/100] Train ACC: 0.549346 Loss: 1.503774 | Val ACC: 0.539570 loss: 1.539743\n",
            "Saving model (epoch =   23, acc = 0.5396)\n",
            "Epoch [24/100], Step [500/3103], Loss: 1.4300\n",
            "Epoch [24/100], Step [1000/3103], Loss: 1.6013\n",
            "Epoch [24/100], Step [1500/3103], Loss: 1.4586\n",
            "Epoch [24/100], Step [2000/3103], Loss: 1.5905\n",
            "Epoch [24/100], Step [2500/3103], Loss: 1.5142\n",
            "Epoch [24/100], Step [3000/3103], Loss: 1.4886\n",
            "[024/100] Train ACC: 0.550576 Loss: 1.498157 | Val ACC: 0.540431 loss: 1.535946\n",
            "Saving model (epoch =   24, acc = 0.5404)\n",
            "Epoch [25/100], Step [500/3103], Loss: 1.5257\n",
            "Epoch [25/100], Step [1000/3103], Loss: 1.5766\n",
            "Epoch [25/100], Step [1500/3103], Loss: 1.4790\n",
            "Epoch [25/100], Step [2000/3103], Loss: 1.4674\n",
            "Epoch [25/100], Step [2500/3103], Loss: 1.4151\n",
            "Epoch [25/100], Step [3000/3103], Loss: 1.5309\n",
            "[025/100] Train ACC: 0.552216 Loss: 1.492678 | Val ACC: 0.539613 loss: 1.536083\n",
            "Epoch [26/100], Step [500/3103], Loss: 1.4948\n",
            "Epoch [26/100], Step [1000/3103], Loss: 1.4974\n",
            "Epoch [26/100], Step [1500/3103], Loss: 1.4421\n",
            "Epoch [26/100], Step [2000/3103], Loss: 1.5448\n",
            "Epoch [26/100], Step [2500/3103], Loss: 1.4642\n",
            "Epoch [26/100], Step [3000/3103], Loss: 1.4837\n",
            "[026/100] Train ACC: 0.553478 Loss: 1.487441 | Val ACC: 0.541274 loss: 1.530070\n",
            "Saving model (epoch =   26, acc = 0.5413)\n",
            "Epoch [27/100], Step [500/3103], Loss: 1.4349\n",
            "Epoch [27/100], Step [1000/3103], Loss: 1.4262\n",
            "Epoch [27/100], Step [1500/3103], Loss: 1.5114\n",
            "Epoch [27/100], Step [2000/3103], Loss: 1.4833\n",
            "Epoch [27/100], Step [2500/3103], Loss: 1.5850\n",
            "Epoch [27/100], Step [3000/3103], Loss: 1.4238\n",
            "[027/100] Train ACC: 0.555002 Loss: 1.482526 | Val ACC: 0.543118 loss: 1.525805\n",
            "Saving model (epoch =   27, acc = 0.5431)\n",
            "Epoch [28/100], Step [500/3103], Loss: 1.4792\n",
            "Epoch [28/100], Step [1000/3103], Loss: 1.4979\n",
            "Epoch [28/100], Step [1500/3103], Loss: 1.4708\n",
            "Epoch [28/100], Step [2000/3103], Loss: 1.4786\n",
            "Epoch [28/100], Step [2500/3103], Loss: 1.5169\n",
            "Epoch [28/100], Step [3000/3103], Loss: 1.4659\n",
            "[028/100] Train ACC: 0.555977 Loss: 1.477676 | Val ACC: 0.543502 loss: 1.523477\n",
            "Saving model (epoch =   28, acc = 0.5435)\n",
            "Epoch [29/100], Step [500/3103], Loss: 1.4300\n",
            "Epoch [29/100], Step [1000/3103], Loss: 1.5492\n",
            "Epoch [29/100], Step [1500/3103], Loss: 1.4633\n",
            "Epoch [29/100], Step [2000/3103], Loss: 1.4140\n",
            "Epoch [29/100], Step [2500/3103], Loss: 1.4736\n",
            "Epoch [29/100], Step [3000/3103], Loss: 1.4806\n",
            "[029/100] Train ACC: 0.557129 Loss: 1.473136 | Val ACC: 0.544937 loss: 1.518291\n",
            "Saving model (epoch =   29, acc = 0.5449)\n",
            "Epoch [30/100], Step [500/3103], Loss: 1.4015\n",
            "Epoch [30/100], Step [1000/3103], Loss: 1.4701\n",
            "Epoch [30/100], Step [1500/3103], Loss: 1.5854\n",
            "Epoch [30/100], Step [2000/3103], Loss: 1.3872\n",
            "Epoch [30/100], Step [2500/3103], Loss: 1.4824\n",
            "Epoch [30/100], Step [3000/3103], Loss: 1.4151\n",
            "[030/100] Train ACC: 0.558188 Loss: 1.468863 | Val ACC: 0.545077 loss: 1.515267\n",
            "Saving model (epoch =   30, acc = 0.5451)\n",
            "Epoch [31/100], Step [500/3103], Loss: 1.4419\n",
            "Epoch [31/100], Step [1000/3103], Loss: 1.5300\n",
            "Epoch [31/100], Step [1500/3103], Loss: 1.4632\n",
            "Epoch [31/100], Step [2000/3103], Loss: 1.5408\n",
            "Epoch [31/100], Step [2500/3103], Loss: 1.4829\n",
            "Epoch [31/100], Step [3000/3103], Loss: 1.4383\n",
            "[031/100] Train ACC: 0.559441 Loss: 1.464530 | Val ACC: 0.545469 loss: 1.515368\n",
            "Saving model (epoch =   31, acc = 0.5455)\n",
            "Epoch [32/100], Step [500/3103], Loss: 1.4499\n",
            "Epoch [32/100], Step [1000/3103], Loss: 1.4726\n",
            "Epoch [32/100], Step [1500/3103], Loss: 1.4405\n",
            "Epoch [32/100], Step [2000/3103], Loss: 1.4499\n",
            "Epoch [32/100], Step [2500/3103], Loss: 1.5317\n",
            "Epoch [32/100], Step [3000/3103], Loss: 1.5027\n",
            "[032/100] Train ACC: 0.560427 Loss: 1.460403 | Val ACC: 0.545988 loss: 1.513336\n",
            "Saving model (epoch =   32, acc = 0.5460)\n",
            "Epoch [33/100], Step [500/3103], Loss: 1.3381\n",
            "Epoch [33/100], Step [1000/3103], Loss: 1.4511\n",
            "Epoch [33/100], Step [1500/3103], Loss: 1.5261\n",
            "Epoch [33/100], Step [2000/3103], Loss: 1.4309\n",
            "Epoch [33/100], Step [2500/3103], Loss: 1.5347\n",
            "Epoch [33/100], Step [3000/3103], Loss: 1.3625\n",
            "[033/100] Train ACC: 0.561325 Loss: 1.456514 | Val ACC: 0.546836 loss: 1.508810\n",
            "Saving model (epoch =   33, acc = 0.5468)\n",
            "Epoch [34/100], Step [500/3103], Loss: 1.5516\n",
            "Epoch [34/100], Step [1000/3103], Loss: 1.4580\n",
            "Epoch [34/100], Step [1500/3103], Loss: 1.5130\n",
            "Epoch [34/100], Step [2000/3103], Loss: 1.3396\n",
            "Epoch [34/100], Step [2500/3103], Loss: 1.4980\n",
            "Epoch [34/100], Step [3000/3103], Loss: 1.3977\n",
            "[034/100] Train ACC: 0.561973 Loss: 1.452764 | Val ACC: 0.547118 loss: 1.506809\n",
            "Saving model (epoch =   34, acc = 0.5471)\n",
            "Epoch [35/100], Step [500/3103], Loss: 1.4225\n",
            "Epoch [35/100], Step [1000/3103], Loss: 1.4372\n",
            "Epoch [35/100], Step [1500/3103], Loss: 1.5541\n",
            "Epoch [35/100], Step [2000/3103], Loss: 1.4644\n",
            "Epoch [35/100], Step [2500/3103], Loss: 1.4943\n",
            "Epoch [35/100], Step [3000/3103], Loss: 1.3667\n",
            "[035/100] Train ACC: 0.563154 Loss: 1.448901 | Val ACC: 0.548629 loss: 1.503065\n",
            "Saving model (epoch =   35, acc = 0.5486)\n",
            "Epoch [36/100], Step [500/3103], Loss: 1.5239\n",
            "Epoch [36/100], Step [1000/3103], Loss: 1.4098\n",
            "Epoch [36/100], Step [1500/3103], Loss: 1.5230\n",
            "Epoch [36/100], Step [2000/3103], Loss: 1.4323\n",
            "Epoch [36/100], Step [2500/3103], Loss: 1.4002\n",
            "Epoch [36/100], Step [3000/3103], Loss: 1.4387\n",
            "[036/100] Train ACC: 0.563951 Loss: 1.445340 | Val ACC: 0.548222 loss: 1.504584\n",
            "Epoch [37/100], Step [500/3103], Loss: 1.5599\n",
            "Epoch [37/100], Step [1000/3103], Loss: 1.5069\n",
            "Epoch [37/100], Step [1500/3103], Loss: 1.4763\n",
            "Epoch [37/100], Step [2000/3103], Loss: 1.3956\n",
            "Epoch [37/100], Step [2500/3103], Loss: 1.5043\n",
            "Epoch [37/100], Step [3000/3103], Loss: 1.4963\n",
            "[037/100] Train ACC: 0.564852 Loss: 1.441672 | Val ACC: 0.549390 loss: 1.499988\n",
            "Saving model (epoch =   37, acc = 0.5494)\n",
            "Epoch [38/100], Step [500/3103], Loss: 1.5131\n",
            "Epoch [38/100], Step [1000/3103], Loss: 1.3802\n",
            "Epoch [38/100], Step [1500/3103], Loss: 1.3685\n",
            "Epoch [38/100], Step [2000/3103], Loss: 1.4783\n",
            "Epoch [38/100], Step [2500/3103], Loss: 1.4202\n",
            "Epoch [38/100], Step [3000/3103], Loss: 1.4504\n",
            "[038/100] Train ACC: 0.565770 Loss: 1.438553 | Val ACC: 0.549237 loss: 1.500875\n",
            "Epoch [39/100], Step [500/3103], Loss: 1.3916\n",
            "Epoch [39/100], Step [1000/3103], Loss: 1.4773\n",
            "Epoch [39/100], Step [1500/3103], Loss: 1.4715\n",
            "Epoch [39/100], Step [2000/3103], Loss: 1.3909\n",
            "Epoch [39/100], Step [2500/3103], Loss: 1.4469\n",
            "Epoch [39/100], Step [3000/3103], Loss: 1.4016\n",
            "[039/100] Train ACC: 0.566524 Loss: 1.435316 | Val ACC: 0.549080 loss: 1.498045\n",
            "Epoch [40/100], Step [500/3103], Loss: 1.4162\n",
            "Epoch [40/100], Step [1000/3103], Loss: 1.3940\n",
            "Epoch [40/100], Step [1500/3103], Loss: 1.5712\n",
            "Epoch [40/100], Step [2000/3103], Loss: 1.4117\n",
            "Epoch [40/100], Step [2500/3103], Loss: 1.4293\n",
            "Epoch [40/100], Step [3000/3103], Loss: 1.4888\n",
            "[040/100] Train ACC: 0.567600 Loss: 1.431968 | Val ACC: 0.549697 loss: 1.497498\n",
            "Saving model (epoch =   40, acc = 0.5497)\n",
            "Epoch [41/100], Step [500/3103], Loss: 1.3806\n",
            "Epoch [41/100], Step [1000/3103], Loss: 1.5413\n",
            "Epoch [41/100], Step [1500/3103], Loss: 1.4036\n",
            "Epoch [41/100], Step [2000/3103], Loss: 1.5272\n",
            "Epoch [41/100], Step [2500/3103], Loss: 1.4277\n",
            "Epoch [41/100], Step [3000/3103], Loss: 1.4640\n",
            "[041/100] Train ACC: 0.568292 Loss: 1.428939 | Val ACC: 0.550261 loss: 1.494689\n",
            "Saving model (epoch =   41, acc = 0.5503)\n",
            "Epoch [42/100], Step [500/3103], Loss: 1.4023\n",
            "Epoch [42/100], Step [1000/3103], Loss: 1.3595\n",
            "Epoch [42/100], Step [1500/3103], Loss: 1.2121\n",
            "Epoch [42/100], Step [2000/3103], Loss: 1.3883\n",
            "Epoch [42/100], Step [2500/3103], Loss: 1.5203\n",
            "Epoch [42/100], Step [3000/3103], Loss: 1.4351\n",
            "[042/100] Train ACC: 0.569257 Loss: 1.425758 | Val ACC: 0.550617 loss: 1.494764\n",
            "Saving model (epoch =   42, acc = 0.5506)\n",
            "Epoch [43/100], Step [500/3103], Loss: 1.3594\n",
            "Epoch [43/100], Step [1000/3103], Loss: 1.4867\n",
            "Epoch [43/100], Step [1500/3103], Loss: 1.3129\n",
            "Epoch [43/100], Step [2000/3103], Loss: 1.5011\n",
            "Epoch [43/100], Step [2500/3103], Loss: 1.4225\n",
            "Epoch [43/100], Step [3000/3103], Loss: 1.4831\n",
            "[043/100] Train ACC: 0.569961 Loss: 1.422708 | Val ACC: 0.551552 loss: 1.492242\n",
            "Saving model (epoch =   43, acc = 0.5516)\n",
            "Epoch [44/100], Step [500/3103], Loss: 1.4522\n",
            "Epoch [44/100], Step [1000/3103], Loss: 1.3819\n",
            "Epoch [44/100], Step [1500/3103], Loss: 1.4558\n",
            "Epoch [44/100], Step [2000/3103], Loss: 1.4582\n",
            "Epoch [44/100], Step [2500/3103], Loss: 1.3132\n",
            "Epoch [44/100], Step [3000/3103], Loss: 1.3476\n",
            "[044/100] Train ACC: 0.570655 Loss: 1.419691 | Val ACC: 0.552275 loss: 1.487920\n",
            "Saving model (epoch =   44, acc = 0.5523)\n",
            "Epoch [45/100], Step [500/3103], Loss: 1.4975\n",
            "Epoch [45/100], Step [1000/3103], Loss: 1.4214\n",
            "Epoch [45/100], Step [1500/3103], Loss: 1.4797\n",
            "Epoch [45/100], Step [2000/3103], Loss: 1.4770\n",
            "Epoch [45/100], Step [2500/3103], Loss: 1.5083\n",
            "Epoch [45/100], Step [3000/3103], Loss: 1.4390\n",
            "[045/100] Train ACC: 0.571521 Loss: 1.417145 | Val ACC: 0.552237 loss: 1.489304\n",
            "Epoch [46/100], Step [500/3103], Loss: 1.3105\n",
            "Epoch [46/100], Step [1000/3103], Loss: 1.4460\n",
            "Epoch [46/100], Step [1500/3103], Loss: 1.4284\n",
            "Epoch [46/100], Step [2000/3103], Loss: 1.4955\n",
            "Epoch [46/100], Step [2500/3103], Loss: 1.5235\n",
            "Epoch [46/100], Step [3000/3103], Loss: 1.3654\n",
            "[046/100] Train ACC: 0.571920 Loss: 1.414360 | Val ACC: 0.552192 loss: 1.489677\n",
            "Epoch [47/100], Step [500/3103], Loss: 1.2706\n",
            "Epoch [47/100], Step [1000/3103], Loss: 1.3811\n",
            "Epoch [47/100], Step [1500/3103], Loss: 1.3507\n",
            "Epoch [47/100], Step [2000/3103], Loss: 1.4699\n",
            "Epoch [47/100], Step [2500/3103], Loss: 1.3955\n",
            "Epoch [47/100], Step [3000/3103], Loss: 1.5349\n",
            "[047/100] Train ACC: 0.572735 Loss: 1.411266 | Val ACC: 0.552807 loss: 1.485408\n",
            "Saving model (epoch =   47, acc = 0.5528)\n",
            "Epoch [48/100], Step [500/3103], Loss: 1.3719\n",
            "Epoch [48/100], Step [1000/3103], Loss: 1.4820\n",
            "Epoch [48/100], Step [1500/3103], Loss: 1.3813\n",
            "Epoch [48/100], Step [2000/3103], Loss: 1.3955\n",
            "Epoch [48/100], Step [2500/3103], Loss: 1.4115\n",
            "Epoch [48/100], Step [3000/3103], Loss: 1.4016\n",
            "[048/100] Train ACC: 0.573385 Loss: 1.408843 | Val ACC: 0.552739 loss: 1.486740\n",
            "Epoch [49/100], Step [500/3103], Loss: 1.3253\n",
            "Epoch [49/100], Step [1000/3103], Loss: 1.3740\n",
            "Epoch [49/100], Step [1500/3103], Loss: 1.3784\n",
            "Epoch [49/100], Step [2000/3103], Loss: 1.4202\n",
            "Epoch [49/100], Step [2500/3103], Loss: 1.3912\n",
            "Epoch [49/100], Step [3000/3103], Loss: 1.3694\n",
            "[049/100] Train ACC: 0.574273 Loss: 1.406190 | Val ACC: 0.552336 loss: 1.486599\n",
            "Epoch [50/100], Step [500/3103], Loss: 1.3333\n",
            "Epoch [50/100], Step [1000/3103], Loss: 1.4133\n",
            "Epoch [50/100], Step [1500/3103], Loss: 1.3032\n",
            "Epoch [50/100], Step [2000/3103], Loss: 1.3944\n",
            "Epoch [50/100], Step [2500/3103], Loss: 1.3481\n",
            "Epoch [50/100], Step [3000/3103], Loss: 1.3401\n",
            "[050/100] Train ACC: 0.575014 Loss: 1.403626 | Val ACC: 0.554227 loss: 1.481715\n",
            "Saving model (epoch =   50, acc = 0.5542)\n",
            "Epoch [51/100], Step [500/3103], Loss: 1.3793\n",
            "Epoch [51/100], Step [1000/3103], Loss: 1.3577\n",
            "Epoch [51/100], Step [1500/3103], Loss: 1.4958\n",
            "Epoch [51/100], Step [2000/3103], Loss: 1.4327\n",
            "Epoch [51/100], Step [2500/3103], Loss: 1.3410\n",
            "Epoch [51/100], Step [3000/3103], Loss: 1.3287\n",
            "[051/100] Train ACC: 0.575474 Loss: 1.401017 | Val ACC: 0.553127 loss: 1.484072\n",
            "Epoch [52/100], Step [500/3103], Loss: 1.4819\n",
            "Epoch [52/100], Step [1000/3103], Loss: 1.3544\n",
            "Epoch [52/100], Step [1500/3103], Loss: 1.3997\n",
            "Epoch [52/100], Step [2000/3103], Loss: 1.5112\n",
            "Epoch [52/100], Step [2500/3103], Loss: 1.3616\n",
            "Epoch [52/100], Step [3000/3103], Loss: 1.4401\n",
            "[052/100] Train ACC: 0.576073 Loss: 1.398599 | Val ACC: 0.554220 loss: 1.482256\n",
            "Epoch [53/100], Step [500/3103], Loss: 1.3206\n",
            "Epoch [53/100], Step [1000/3103], Loss: 1.3734\n",
            "Epoch [53/100], Step [1500/3103], Loss: 1.3920\n",
            "Epoch [53/100], Step [2000/3103], Loss: 1.4381\n",
            "Epoch [53/100], Step [2500/3103], Loss: 1.3855\n",
            "Epoch [53/100], Step [3000/3103], Loss: 1.3271\n",
            "[053/100] Train ACC: 0.576925 Loss: 1.396068 | Val ACC: 0.554496 loss: 1.480982\n",
            "Saving model (epoch =   53, acc = 0.5545)\n",
            "Epoch [54/100], Step [500/3103], Loss: 1.3310\n",
            "Epoch [54/100], Step [1000/3103], Loss: 1.3988\n",
            "Epoch [54/100], Step [1500/3103], Loss: 1.4496\n",
            "Epoch [54/100], Step [2000/3103], Loss: 1.3384\n",
            "Epoch [54/100], Step [2500/3103], Loss: 1.3339\n",
            "Epoch [54/100], Step [3000/3103], Loss: 1.5128\n",
            "[054/100] Train ACC: 0.577595 Loss: 1.393574 | Val ACC: 0.554178 loss: 1.480625\n",
            "Epoch [55/100], Step [500/3103], Loss: 1.3437\n",
            "Epoch [55/100], Step [1000/3103], Loss: 1.2850\n",
            "Epoch [55/100], Step [1500/3103], Loss: 1.3831\n",
            "Epoch [55/100], Step [2000/3103], Loss: 1.5122\n",
            "Epoch [55/100], Step [2500/3103], Loss: 1.3366\n",
            "Epoch [55/100], Step [3000/3103], Loss: 1.3537\n",
            "[055/100] Train ACC: 0.578179 Loss: 1.391255 | Val ACC: 0.554837 loss: 1.478015\n",
            "Saving model (epoch =   55, acc = 0.5548)\n",
            "Epoch [56/100], Step [500/3103], Loss: 1.3255\n",
            "Epoch [56/100], Step [1000/3103], Loss: 1.3934\n",
            "Epoch [56/100], Step [1500/3103], Loss: 1.4623\n",
            "Epoch [56/100], Step [2000/3103], Loss: 1.2525\n",
            "Epoch [56/100], Step [2500/3103], Loss: 1.3656\n",
            "Epoch [56/100], Step [3000/3103], Loss: 1.4001\n",
            "[056/100] Train ACC: 0.578649 Loss: 1.389097 | Val ACC: 0.554388 loss: 1.479128\n",
            "Epoch [57/100], Step [500/3103], Loss: 1.4053\n",
            "Epoch [57/100], Step [1000/3103], Loss: 1.4896\n",
            "Epoch [57/100], Step [1500/3103], Loss: 1.4885\n",
            "Epoch [57/100], Step [2000/3103], Loss: 1.3362\n",
            "Epoch [57/100], Step [2500/3103], Loss: 1.4246\n",
            "Epoch [57/100], Step [3000/3103], Loss: 1.3788\n",
            "[057/100] Train ACC: 0.579401 Loss: 1.386695 | Val ACC: 0.555802 loss: 1.473826\n",
            "Saving model (epoch =   57, acc = 0.5558)\n",
            "Epoch [58/100], Step [500/3103], Loss: 1.3681\n",
            "Epoch [58/100], Step [1000/3103], Loss: 1.3836\n",
            "Epoch [58/100], Step [1500/3103], Loss: 1.3273\n",
            "Epoch [58/100], Step [2000/3103], Loss: 1.4308\n",
            "Epoch [58/100], Step [2500/3103], Loss: 1.2445\n",
            "Epoch [58/100], Step [3000/3103], Loss: 1.3713\n",
            "[058/100] Train ACC: 0.579927 Loss: 1.384557 | Val ACC: 0.554602 loss: 1.477728\n",
            "Epoch [59/100], Step [500/3103], Loss: 1.3398\n",
            "Epoch [59/100], Step [1000/3103], Loss: 1.4432\n",
            "Epoch [59/100], Step [1500/3103], Loss: 1.4562\n",
            "Epoch [59/100], Step [2000/3103], Loss: 1.3090\n",
            "Epoch [59/100], Step [2500/3103], Loss: 1.3842\n",
            "Epoch [59/100], Step [3000/3103], Loss: 1.4585\n",
            "[059/100] Train ACC: 0.580412 Loss: 1.382346 | Val ACC: 0.555249 loss: 1.477986\n",
            "Epoch [60/100], Step [500/3103], Loss: 1.3502\n",
            "Epoch [60/100], Step [1000/3103], Loss: 1.5302\n",
            "Epoch [60/100], Step [1500/3103], Loss: 1.4134\n",
            "Epoch [60/100], Step [2000/3103], Loss: 1.4041\n",
            "Epoch [60/100], Step [2500/3103], Loss: 1.3412\n",
            "Epoch [60/100], Step [3000/3103], Loss: 1.3656\n",
            "[060/100] Train ACC: 0.580894 Loss: 1.380408 | Val ACC: 0.555575 loss: 1.474837\n",
            "Epoch [61/100], Step [500/3103], Loss: 1.3784\n",
            "Epoch [61/100], Step [1000/3103], Loss: 1.3135\n",
            "Epoch [61/100], Step [1500/3103], Loss: 1.3844\n",
            "Epoch [61/100], Step [2000/3103], Loss: 1.4219\n",
            "Epoch [61/100], Step [2500/3103], Loss: 1.3062\n",
            "Epoch [61/100], Step [3000/3103], Loss: 1.3144\n",
            "[061/100] Train ACC: 0.581635 Loss: 1.377984 | Val ACC: 0.555352 loss: 1.476015\n",
            "Epoch [62/100], Step [500/3103], Loss: 1.4204\n",
            "Epoch [62/100], Step [1000/3103], Loss: 1.3330\n",
            "Epoch [62/100], Step [1500/3103], Loss: 1.3960\n",
            "Epoch [62/100], Step [2000/3103], Loss: 1.4394\n",
            "Epoch [62/100], Step [2500/3103], Loss: 1.4558\n",
            "Epoch [62/100], Step [3000/3103], Loss: 1.3583\n",
            "[062/100] Train ACC: 0.581979 Loss: 1.375698 | Val ACC: 0.556052 loss: 1.474462\n",
            "Saving model (epoch =   62, acc = 0.5561)\n",
            "Epoch [63/100], Step [500/3103], Loss: 1.3082\n",
            "Epoch [63/100], Step [1000/3103], Loss: 1.2772\n",
            "Epoch [63/100], Step [1500/3103], Loss: 1.3941\n",
            "Epoch [63/100], Step [2000/3103], Loss: 1.3270\n",
            "Epoch [63/100], Step [2500/3103], Loss: 1.4640\n",
            "Epoch [63/100], Step [3000/3103], Loss: 1.4272\n",
            "[063/100] Train ACC: 0.582724 Loss: 1.373915 | Val ACC: 0.556052 loss: 1.472674\n",
            "Epoch [64/100], Step [500/3103], Loss: 1.3557\n",
            "Epoch [64/100], Step [1000/3103], Loss: 1.5489\n",
            "Epoch [64/100], Step [1500/3103], Loss: 1.3671\n",
            "Epoch [64/100], Step [2000/3103], Loss: 1.4527\n",
            "Epoch [64/100], Step [2500/3103], Loss: 1.3775\n",
            "Epoch [64/100], Step [3000/3103], Loss: 1.4005\n",
            "[064/100] Train ACC: 0.583297 Loss: 1.371697 | Val ACC: 0.556098 loss: 1.473638\n",
            "Saving model (epoch =   64, acc = 0.5561)\n",
            "Epoch [65/100], Step [500/3103], Loss: 1.4376\n",
            "Epoch [65/100], Step [1000/3103], Loss: 1.4779\n",
            "Epoch [65/100], Step [1500/3103], Loss: 1.2694\n",
            "Epoch [65/100], Step [2000/3103], Loss: 1.4336\n",
            "Epoch [65/100], Step [2500/3103], Loss: 1.4518\n",
            "Epoch [65/100], Step [3000/3103], Loss: 1.4059\n",
            "[065/100] Train ACC: 0.583784 Loss: 1.369656 | Val ACC: 0.556478 loss: 1.470826\n",
            "Saving model (epoch =   65, acc = 0.5565)\n",
            "Epoch [66/100], Step [500/3103], Loss: 1.4321\n",
            "Epoch [66/100], Step [1000/3103], Loss: 1.4076\n",
            "Epoch [66/100], Step [1500/3103], Loss: 1.4362\n",
            "Epoch [66/100], Step [2000/3103], Loss: 1.2998\n",
            "Epoch [66/100], Step [2500/3103], Loss: 1.3392\n",
            "Epoch [66/100], Step [3000/3103], Loss: 1.3874\n",
            "[066/100] Train ACC: 0.584399 Loss: 1.367646 | Val ACC: 0.557133 loss: 1.469482\n",
            "Saving model (epoch =   66, acc = 0.5571)\n",
            "Epoch [67/100], Step [500/3103], Loss: 1.4128\n",
            "Epoch [67/100], Step [1000/3103], Loss: 1.3851\n",
            "Epoch [67/100], Step [1500/3103], Loss: 1.3571\n",
            "Epoch [67/100], Step [2000/3103], Loss: 1.2498\n",
            "Epoch [67/100], Step [2500/3103], Loss: 1.4138\n",
            "Epoch [67/100], Step [3000/3103], Loss: 1.4492\n",
            "[067/100] Train ACC: 0.584863 Loss: 1.365690 | Val ACC: 0.556145 loss: 1.472169\n",
            "Epoch [68/100], Step [500/3103], Loss: 1.3997\n",
            "Epoch [68/100], Step [1000/3103], Loss: 1.3832\n",
            "Epoch [68/100], Step [1500/3103], Loss: 1.3696\n",
            "Epoch [68/100], Step [2000/3103], Loss: 1.4128\n",
            "Epoch [68/100], Step [2500/3103], Loss: 1.4282\n",
            "Epoch [68/100], Step [3000/3103], Loss: 1.3878\n",
            "[068/100] Train ACC: 0.585434 Loss: 1.363593 | Val ACC: 0.556323 loss: 1.471470\n",
            "Epoch [69/100], Step [500/3103], Loss: 1.3704\n",
            "Epoch [69/100], Step [1000/3103], Loss: 1.3669\n",
            "Epoch [69/100], Step [1500/3103], Loss: 1.3533\n",
            "Epoch [69/100], Step [2000/3103], Loss: 1.3856\n",
            "Epoch [69/100], Step [2500/3103], Loss: 1.2365\n",
            "Epoch [69/100], Step [3000/3103], Loss: 1.2864\n",
            "[069/100] Train ACC: 0.585796 Loss: 1.361792 | Val ACC: 0.556916 loss: 1.470046\n",
            "Epoch [70/100], Step [500/3103], Loss: 1.3965\n",
            "Epoch [70/100], Step [1000/3103], Loss: 1.3149\n",
            "Epoch [70/100], Step [1500/3103], Loss: 1.3877\n",
            "Epoch [70/100], Step [2000/3103], Loss: 1.3838\n",
            "Epoch [70/100], Step [2500/3103], Loss: 1.4468\n",
            "Epoch [70/100], Step [3000/3103], Loss: 1.2494\n",
            "[070/100] Train ACC: 0.586354 Loss: 1.359757 | Val ACC: 0.557622 loss: 1.467907\n",
            "Saving model (epoch =   70, acc = 0.5576)\n",
            "Epoch [71/100], Step [500/3103], Loss: 1.3424\n",
            "Epoch [71/100], Step [1000/3103], Loss: 1.3896\n",
            "Epoch [71/100], Step [1500/3103], Loss: 1.3795\n",
            "Epoch [71/100], Step [2000/3103], Loss: 1.4414\n",
            "Epoch [71/100], Step [2500/3103], Loss: 1.3159\n",
            "Epoch [71/100], Step [3000/3103], Loss: 1.3453\n",
            "[071/100] Train ACC: 0.587046 Loss: 1.357833 | Val ACC: 0.557046 loss: 1.470737\n",
            "Epoch [72/100], Step [500/3103], Loss: 1.4509\n",
            "Epoch [72/100], Step [1000/3103], Loss: 1.4251\n",
            "Epoch [72/100], Step [1500/3103], Loss: 1.3612\n",
            "Epoch [72/100], Step [2000/3103], Loss: 1.4211\n",
            "Epoch [72/100], Step [2500/3103], Loss: 1.2992\n",
            "Epoch [72/100], Step [3000/3103], Loss: 1.5010\n",
            "[072/100] Train ACC: 0.587572 Loss: 1.355900 | Val ACC: 0.557743 loss: 1.468814\n",
            "Saving model (epoch =   72, acc = 0.5577)\n",
            "Epoch [73/100], Step [500/3103], Loss: 1.3399\n",
            "Epoch [73/100], Step [1000/3103], Loss: 1.2830\n",
            "Epoch [73/100], Step [1500/3103], Loss: 1.3728\n",
            "Epoch [73/100], Step [2000/3103], Loss: 1.3739\n",
            "Epoch [73/100], Step [2500/3103], Loss: 1.3940\n",
            "Epoch [73/100], Step [3000/3103], Loss: 1.4538\n",
            "[073/100] Train ACC: 0.587925 Loss: 1.354086 | Val ACC: 0.556972 loss: 1.471008\n",
            "Epoch [74/100], Step [500/3103], Loss: 1.4691\n",
            "Epoch [74/100], Step [1000/3103], Loss: 1.2982\n",
            "Epoch [74/100], Step [1500/3103], Loss: 1.4232\n",
            "Epoch [74/100], Step [2000/3103], Loss: 1.2414\n",
            "Epoch [74/100], Step [2500/3103], Loss: 1.3704\n",
            "Epoch [74/100], Step [3000/3103], Loss: 1.2746\n",
            "[074/100] Train ACC: 0.588335 Loss: 1.352227 | Val ACC: 0.557353 loss: 1.469933\n",
            "Epoch [75/100], Step [500/3103], Loss: 1.3613\n",
            "Epoch [75/100], Step [1000/3103], Loss: 1.2478\n",
            "Epoch [75/100], Step [1500/3103], Loss: 1.4406\n",
            "Epoch [75/100], Step [2000/3103], Loss: 1.3553\n",
            "Epoch [75/100], Step [2500/3103], Loss: 1.3376\n",
            "Epoch [75/100], Step [3000/3103], Loss: 1.3063\n",
            "[075/100] Train ACC: 0.588856 Loss: 1.350662 | Val ACC: 0.557372 loss: 1.472101\n",
            "Epoch [76/100], Step [500/3103], Loss: 1.3844\n",
            "Epoch [76/100], Step [1000/3103], Loss: 1.3522\n",
            "Epoch [76/100], Step [1500/3103], Loss: 1.2731\n",
            "Epoch [76/100], Step [2000/3103], Loss: 1.4439\n",
            "Epoch [76/100], Step [2500/3103], Loss: 1.3508\n",
            "Epoch [76/100], Step [3000/3103], Loss: 1.2354\n",
            "[076/100] Train ACC: 0.589351 Loss: 1.348664 | Val ACC: 0.558214 loss: 1.466817\n",
            "Saving model (epoch =   76, acc = 0.5582)\n",
            "Epoch [77/100], Step [500/3103], Loss: 1.3480\n",
            "Epoch [77/100], Step [1000/3103], Loss: 1.1976\n",
            "Epoch [77/100], Step [1500/3103], Loss: 1.4143\n",
            "Epoch [77/100], Step [2000/3103], Loss: 1.3206\n",
            "Epoch [77/100], Step [2500/3103], Loss: 1.2815\n",
            "Epoch [77/100], Step [3000/3103], Loss: 1.2775\n",
            "[077/100] Train ACC: 0.589934 Loss: 1.346846 | Val ACC: 0.557578 loss: 1.469122\n",
            "Epoch [78/100], Step [500/3103], Loss: 1.2511\n",
            "Epoch [78/100], Step [1000/3103], Loss: 1.2725\n",
            "Epoch [78/100], Step [1500/3103], Loss: 1.3180\n",
            "Epoch [78/100], Step [2000/3103], Loss: 1.3234\n",
            "Epoch [78/100], Step [2500/3103], Loss: 1.2694\n",
            "Epoch [78/100], Step [3000/3103], Loss: 1.3953\n",
            "[078/100] Train ACC: 0.590517 Loss: 1.344972 | Val ACC: 0.558114 loss: 1.466299\n",
            "Epoch [79/100], Step [500/3103], Loss: 1.3741\n",
            "Epoch [79/100], Step [1000/3103], Loss: 1.2579\n",
            "Epoch [79/100], Step [1500/3103], Loss: 1.3961\n",
            "Epoch [79/100], Step [2000/3103], Loss: 1.2736\n",
            "Epoch [79/100], Step [2500/3103], Loss: 1.3051\n",
            "Epoch [79/100], Step [3000/3103], Loss: 1.3624\n",
            "[079/100] Train ACC: 0.590801 Loss: 1.343289 | Val ACC: 0.558750 loss: 1.465222\n",
            "Saving model (epoch =   79, acc = 0.5588)\n",
            "Epoch [80/100], Step [500/3103], Loss: 1.3533\n",
            "Epoch [80/100], Step [1000/3103], Loss: 1.2230\n",
            "Epoch [80/100], Step [1500/3103], Loss: 1.2302\n",
            "Epoch [80/100], Step [2000/3103], Loss: 1.2829\n",
            "Epoch [80/100], Step [2500/3103], Loss: 1.3596\n",
            "Epoch [80/100], Step [3000/3103], Loss: 1.3592\n",
            "[080/100] Train ACC: 0.591375 Loss: 1.341458 | Val ACC: 0.557306 loss: 1.468534\n",
            "Epoch [81/100], Step [500/3103], Loss: 1.4050\n",
            "Epoch [81/100], Step [1000/3103], Loss: 1.3412\n",
            "Epoch [81/100], Step [1500/3103], Loss: 1.2739\n",
            "Epoch [81/100], Step [2000/3103], Loss: 1.3328\n",
            "Epoch [81/100], Step [2500/3103], Loss: 1.2983\n",
            "Epoch [81/100], Step [3000/3103], Loss: 1.3501\n",
            "[081/100] Train ACC: 0.591829 Loss: 1.339897 | Val ACC: 0.557063 loss: 1.469665\n",
            "Epoch [82/100], Step [500/3103], Loss: 1.3242\n",
            "Epoch [82/100], Step [1000/3103], Loss: 1.3840\n",
            "Epoch [82/100], Step [1500/3103], Loss: 1.3373\n",
            "Epoch [82/100], Step [2000/3103], Loss: 1.1902\n",
            "Epoch [82/100], Step [2500/3103], Loss: 1.3575\n",
            "Epoch [82/100], Step [3000/3103], Loss: 1.2701\n",
            "[082/100] Train ACC: 0.592568 Loss: 1.338011 | Val ACC: 0.556603 loss: 1.472204\n",
            "Epoch [83/100], Step [500/3103], Loss: 1.3819\n",
            "Epoch [83/100], Step [1000/3103], Loss: 1.2249\n",
            "Epoch [83/100], Step [1500/3103], Loss: 1.4039\n",
            "Epoch [83/100], Step [2000/3103], Loss: 1.3256\n",
            "Epoch [83/100], Step [2500/3103], Loss: 1.3544\n",
            "Epoch [83/100], Step [3000/3103], Loss: 1.4342\n",
            "[083/100] Train ACC: 0.592734 Loss: 1.336502 | Val ACC: 0.557531 loss: 1.469554\n",
            "Epoch [84/100], Step [500/3103], Loss: 1.2291\n",
            "Epoch [84/100], Step [1000/3103], Loss: 1.3138\n",
            "Epoch [84/100], Step [1500/3103], Loss: 1.3449\n",
            "Epoch [84/100], Step [2000/3103], Loss: 1.4588\n",
            "Epoch [84/100], Step [2500/3103], Loss: 1.2937\n",
            "Epoch [84/100], Step [3000/3103], Loss: 1.3176\n",
            "[084/100] Train ACC: 0.592995 Loss: 1.334696 | Val ACC: 0.558633 loss: 1.465561\n",
            "Epoch [85/100], Step [500/3103], Loss: 1.3922\n",
            "Epoch [85/100], Step [1000/3103], Loss: 1.3041\n",
            "Epoch [85/100], Step [1500/3103], Loss: 1.3880\n",
            "Epoch [85/100], Step [2000/3103], Loss: 1.1806\n",
            "Epoch [85/100], Step [2500/3103], Loss: 1.4024\n",
            "Epoch [85/100], Step [3000/3103], Loss: 1.4404\n",
            "[085/100] Train ACC: 0.593663 Loss: 1.333094 | Val ACC: 0.558341 loss: 1.466961\n",
            "Epoch [86/100], Step [500/3103], Loss: 1.3463\n",
            "Epoch [86/100], Step [1000/3103], Loss: 1.3402\n",
            "Epoch [86/100], Step [1500/3103], Loss: 1.3179\n",
            "Epoch [86/100], Step [2000/3103], Loss: 1.3946\n",
            "Epoch [86/100], Step [2500/3103], Loss: 1.2279\n",
            "Epoch [86/100], Step [3000/3103], Loss: 1.4047\n",
            "[086/100] Train ACC: 0.594503 Loss: 1.331516 | Val ACC: 0.557593 loss: 1.470253\n",
            "Epoch [87/100], Step [500/3103], Loss: 1.2862\n",
            "Epoch [87/100], Step [1000/3103], Loss: 1.3614\n",
            "Epoch [87/100], Step [1500/3103], Loss: 1.4619\n",
            "Epoch [87/100], Step [2000/3103], Loss: 1.3503\n",
            "Epoch [87/100], Step [2500/3103], Loss: 1.3089\n",
            "Epoch [87/100], Step [3000/3103], Loss: 1.3138\n",
            "[087/100] Train ACC: 0.594527 Loss: 1.329698 | Val ACC: 0.557523 loss: 1.469699\n",
            "Epoch [88/100], Step [500/3103], Loss: 1.3532\n",
            "Epoch [88/100], Step [1000/3103], Loss: 1.2960\n",
            "Epoch [88/100], Step [1500/3103], Loss: 1.3764\n",
            "Epoch [88/100], Step [2000/3103], Loss: 1.3338\n",
            "Epoch [88/100], Step [2500/3103], Loss: 1.3899\n",
            "Epoch [88/100], Step [3000/3103], Loss: 1.3332\n",
            "[088/100] Train ACC: 0.595296 Loss: 1.327829 | Val ACC: 0.558042 loss: 1.468585\n",
            "Epoch [89/100], Step [500/3103], Loss: 1.2480\n",
            "Epoch [89/100], Step [1000/3103], Loss: 1.3433\n",
            "Epoch [89/100], Step [1500/3103], Loss: 1.2153\n",
            "Epoch [89/100], Step [2000/3103], Loss: 1.4468\n",
            "Epoch [89/100], Step [2500/3103], Loss: 1.3316\n",
            "Epoch [89/100], Step [3000/3103], Loss: 1.3338\n",
            "[089/100] Train ACC: 0.595607 Loss: 1.326398 | Val ACC: 0.558860 loss: 1.465881\n",
            "Saving model (epoch =   89, acc = 0.5589)\n",
            "Epoch [90/100], Step [500/3103], Loss: 1.4237\n",
            "Epoch [90/100], Step [1000/3103], Loss: 1.3363\n",
            "Epoch [90/100], Step [1500/3103], Loss: 1.2127\n",
            "Epoch [90/100], Step [2000/3103], Loss: 1.2969\n",
            "Epoch [90/100], Step [2500/3103], Loss: 1.3225\n",
            "Epoch [90/100], Step [3000/3103], Loss: 1.2842\n",
            "[090/100] Train ACC: 0.596007 Loss: 1.324643 | Val ACC: 0.559115 loss: 1.466369\n",
            "Saving model (epoch =   90, acc = 0.5591)\n",
            "Epoch [91/100], Step [500/3103], Loss: 1.2997\n",
            "Epoch [91/100], Step [1000/3103], Loss: 1.3237\n",
            "Epoch [91/100], Step [1500/3103], Loss: 1.2207\n",
            "Epoch [91/100], Step [2000/3103], Loss: 1.3757\n",
            "Epoch [91/100], Step [2500/3103], Loss: 1.3289\n",
            "Epoch [91/100], Step [3000/3103], Loss: 1.3743\n",
            "[091/100] Train ACC: 0.596338 Loss: 1.323093 | Val ACC: 0.559025 loss: 1.464583\n",
            "Epoch [92/100], Step [500/3103], Loss: 1.2325\n",
            "Epoch [92/100], Step [1000/3103], Loss: 1.2609\n",
            "Epoch [92/100], Step [1500/3103], Loss: 1.1870\n",
            "Epoch [92/100], Step [2000/3103], Loss: 1.4395\n",
            "Epoch [92/100], Step [2500/3103], Loss: 1.2645\n",
            "Epoch [92/100], Step [3000/3103], Loss: 1.2707\n",
            "[092/100] Train ACC: 0.597021 Loss: 1.321575 | Val ACC: 0.556207 loss: 1.472339\n",
            "Epoch [93/100], Step [500/3103], Loss: 1.2268\n",
            "Epoch [93/100], Step [1000/3103], Loss: 1.2568\n",
            "Epoch [93/100], Step [1500/3103], Loss: 1.2096\n",
            "Epoch [93/100], Step [2000/3103], Loss: 1.2961\n",
            "Epoch [93/100], Step [2500/3103], Loss: 1.2827\n",
            "Epoch [93/100], Step [3000/3103], Loss: 1.3560\n",
            "[093/100] Train ACC: 0.597293 Loss: 1.319959 | Val ACC: 0.558797 loss: 1.464921\n",
            "Epoch [94/100], Step [500/3103], Loss: 1.2671\n",
            "Epoch [94/100], Step [1000/3103], Loss: 1.3577\n",
            "Epoch [94/100], Step [1500/3103], Loss: 1.2145\n",
            "Epoch [94/100], Step [2000/3103], Loss: 1.4403\n",
            "Epoch [94/100], Step [2500/3103], Loss: 1.2860\n",
            "Epoch [94/100], Step [3000/3103], Loss: 1.3404\n",
            "[094/100] Train ACC: 0.597583 Loss: 1.318306 | Val ACC: 0.556435 loss: 1.472954\n",
            "Epoch [95/100], Step [500/3103], Loss: 1.3266\n",
            "Epoch [95/100], Step [1000/3103], Loss: 1.3291\n",
            "Epoch [95/100], Step [1500/3103], Loss: 1.4416\n",
            "Epoch [95/100], Step [2000/3103], Loss: 1.3886\n",
            "Epoch [95/100], Step [2500/3103], Loss: 1.2697\n",
            "Epoch [95/100], Step [3000/3103], Loss: 1.3073\n",
            "[095/100] Train ACC: 0.597975 Loss: 1.316964 | Val ACC: 0.558303 loss: 1.466419\n",
            "Epoch [96/100], Step [500/3103], Loss: 1.3180\n",
            "Epoch [96/100], Step [1000/3103], Loss: 1.2942\n",
            "Epoch [96/100], Step [1500/3103], Loss: 1.2247\n",
            "Epoch [96/100], Step [2000/3103], Loss: 1.2651\n",
            "Epoch [96/100], Step [2500/3103], Loss: 1.4229\n",
            "Epoch [96/100], Step [3000/3103], Loss: 1.2879\n",
            "[096/100] Train ACC: 0.598374 Loss: 1.315372 | Val ACC: 0.558695 loss: 1.467620\n",
            "Epoch [97/100], Step [500/3103], Loss: 1.4483\n",
            "Epoch [97/100], Step [1000/3103], Loss: 1.3536\n",
            "Epoch [97/100], Step [1500/3103], Loss: 1.3226\n",
            "Epoch [97/100], Step [2000/3103], Loss: 1.3530\n",
            "Epoch [97/100], Step [2500/3103], Loss: 1.3796\n",
            "Epoch [97/100], Step [3000/3103], Loss: 1.3548\n",
            "[097/100] Train ACC: 0.598935 Loss: 1.313825 | Val ACC: 0.558388 loss: 1.470720\n",
            "Epoch [98/100], Step [500/3103], Loss: 1.2977\n",
            "Epoch [98/100], Step [1000/3103], Loss: 1.3228\n",
            "Epoch [98/100], Step [1500/3103], Loss: 1.3266\n",
            "Epoch [98/100], Step [2000/3103], Loss: 1.1817\n",
            "Epoch [98/100], Step [2500/3103], Loss: 1.3010\n",
            "Epoch [98/100], Step [3000/3103], Loss: 1.1721\n",
            "[098/100] Train ACC: 0.599411 Loss: 1.312139 | Val ACC: 0.559259 loss: 1.465240\n",
            "Saving model (epoch =   98, acc = 0.5593)\n",
            "Epoch [99/100], Step [500/3103], Loss: 1.3060\n",
            "Epoch [99/100], Step [1000/3103], Loss: 1.2153\n",
            "Epoch [99/100], Step [1500/3103], Loss: 1.2255\n",
            "Epoch [99/100], Step [2000/3103], Loss: 1.2369\n",
            "Epoch [99/100], Step [2500/3103], Loss: 1.3645\n",
            "Epoch [99/100], Step [3000/3103], Loss: 1.2816\n",
            "[099/100] Train ACC: 0.600031 Loss: 1.310584 | Val ACC: 0.558833 loss: 1.466550\n",
            "Epoch [100/100], Step [500/3103], Loss: 1.3343\n",
            "Epoch [100/100], Step [1000/3103], Loss: 1.2446\n",
            "Epoch [100/100], Step [1500/3103], Loss: 1.3636\n",
            "Epoch [100/100], Step [2000/3103], Loss: 1.4154\n",
            "Epoch [100/100], Step [2500/3103], Loss: 1.3400\n",
            "Epoch [100/100], Step [3000/3103], Loss: 1.3641\n",
            "[100/100] Train ACC: 0.599964 Loss: 1.309371 | Val ACC: 0.558652 loss: 1.466735\n",
            "Finished training after 100 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['val'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['val'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DRZ_0wLCnDGl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_acc)\n",
        "plot_learning_curve(loss_record, title='deep model')\n",
        "plot_learning_curve(acc_record, title='deep model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "XE_mjW7WmRVK",
        "outputId": "99aaa636-5946-49e7-e7a6-299ca05a837e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5592593013305466\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_X = preprocess_data(split='test', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes)\n",
        "test_set = VoiceDataset(test_X, None)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "predicts = test(test_loader, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3myVFCPq5xGl",
        "outputId": "942d0620-9e06-4377-ddc6-e7ea8c9ac933"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset] - # phone classes: 41, number of utterances for test: 857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "857it [00:00, 1494.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] test set\n",
            "torch.Size([527364, 117])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predicts):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "metadata": {
        "id": "KydcOA7RBJ3N"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('prediction.csv') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pyycMgzT4ssV",
        "outputId": "739c2ab9-d918-43e6-d978-d4cc3c3c59bc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cd292719-d22b-44bb-a013-578366575e3e\", \"prediction.csv\", 4931884)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}