{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNT2lzuL73SA5jZ+32UICqk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7UniKqPi-Cps","executionInfo":{"status":"ok","timestamp":1678452692160,"user_tz":-480,"elapsed":34689,"user":{"displayName":"helfenstein xiao","userId":"14494537840267835334"}},"outputId":"dc3cb64a-b100-44ef-b384-1daf84fc2b3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.9/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1-WZHD7fL8fCyQHAxxwnEmJJwH73kYua_&confirm=t\n","To: /content/data.zip\n","100% 372M/372M [00:01<00:00, 238MB/s]\n","Archive:  data.zip\n","   creating: timit_11/\n","  inflating: timit_11/train_11.npy   \n","  inflating: timit_11/test_11.npy    \n","  inflating: timit_11/train_label_11.npy  \n","data.zip  sample_data  timit_11\n"]}],"source":["!gdown --id '1-WZHD7fL8fCyQHAxxwnEmJJwH73kYua_&confirm=t' --output data.zip\n","!unzip data.zip\n","!ls "]},{"cell_type":"code","source":["!ls\n","!mkdir models\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCPuZSEjXXKY","executionInfo":{"status":"ok","timestamp":1678452697083,"user_tz":-480,"elapsed":471,"user":{"displayName":"helfenstein xiao","userId":"14494537840267835334"}},"outputId":"1883612a-755b-4539-de2c-1911422b2b2a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["data.zip  sample_data  timit_11\n","data.zip  models  sample_data  timit_11\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import gc\n","\n","# load data\n","data = np.load('timit_11/train_11.npy')  # load training data\n","print(data.shape)  # (1229932, 429)\n","print(data.dtype)\n","\n","label = np.load('timit_11/train_label_11.npy')  # load labels\n","print(label.shape)  # (1229932,)\n","print(label.dtype)\n","print(label[0])\n","print(label[90])\n","\n","del data, label\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8XHpBBZZATq","executionInfo":{"status":"ok","timestamp":1678450471744,"user_tz":-480,"elapsed":2943,"user":{"displayName":"helfenstein xiao","userId":"14494537840267835334"}},"outputId":"cd3d1770-6a28-46b8-fcd0-e83b109da2f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1229932, 429)\n","float64\n","(1229932,)\n","<U2\n","36\n","35\n"]},{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import gc\n","\n","myseed = 42069  # set a random seed for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(myseed)\n","torch.manual_seed(myseed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(myseed)\n","\n","\n","# support gpu or not\n","def get_device():\n","    return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","# split data into training data and validation data\n","def split_data(data, label):\n","    VAL_RATIO = 0.2\n","\n","    percent = int(data.shape[0] * (1 - VAL_RATIO))\n","    train_data, train_label, val_data, val_label = data[:percent], label[:percent], data[percent:], label[percent:]\n","    print('Size of training set: {}'.format(train_data.shape))\n","    print('Size of validation set: {}'.format(val_data.shape))\n","    return train_data, train_label, val_data, val_label\n","\n","\n","# a custom Dataset, load data from .npy files\n","class VoiceDataset(Dataset):\n","    def __init__(self, data_train, data_label):\n","        super().__init__()\n","        self.data_train = torch.from_numpy(data_train).float()  # data for training\n","        self.data_label = torch.LongTensor(data_label.astype(np.int32))  # label for data used for training\n","\n","    def __getitem__(self, index):\n","        return self.data_train[index], self.data_label[index]  # return a element in dataset according to index\n","\n","    def __len__(self):\n","        return len(self.data_label)  # return the length of this dataset\n","\n","\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, output_size) -> None:\n","        super(NeuralNet, self).__init__()\n","        # define the network\n","        self.net = nn.Sequential(\n","            nn.Linear(input_size, 1024),\n","            nn.Sigmoid(),\n","            nn.Linear(1024, 512),\n","            nn.Sigmoid(),\n","            nn.Linear(512, 128),\n","            nn.Sigmoid(),\n","            nn.Linear(128, output_size),\n","        )\n","\n","    def forward(self, input):\n","        # return the output of network\n","        return self.net(input)\n","    \n","    def cal_loss(self, pred, target):\n","        self.criterion = nn.CrossEntropyLoss()  # set loss function to Cross Entropy\n","        return self.criterion(pred, target)\n","\n","\n","# this function includes everything for training\n","def train(tr_set, dv_set, model, config, device):\n","    num_epochs = config['num_epochs']  # number of epochs\n","\n","    # set the optimizer\n","    optimizer = getattr(torch.optim, config['optimizer'])(\n","        model.parameters(), **config['optim_hparas'])\n","    \n","    # init parameters for epochs\n","    min_mse = 1000.  # set the initial value for min_mse (higher than the mse after first epoch)\n","    loss_record = {'train': [], 'dev': []}\n","    \n","    # epochs for trianing\n","    for epoch in range(num_epochs):\n","        model.train()  # set model to trian mode\n","        for i, (data, label) in enumerate(tr_set):\n","            optimizer.zero_grad()  # set gradient to zero before calculate\n","            data, label = data.to(device), label.to(device)  # move data to device\n","            pred = model(data)  # compute the predict from data\n","            loss = model.cal_loss(pred, label)  # compute the mse loss\n","            loss.backward()  # get the gradient\n","            optimizer.step()  # updata parameters in model\n","            loss_record['train'].append(loss.detach().cpu().item())\n","\n","            # print training status every 100 optimisations\n","            if (i+1) % 100 == 0:\n","                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, len(tr_set), loss.item()))\n","\n","        # After each epoch, test your model on the validation (development) set.\n","        dev_mse = dev(dv_set, model, device)\n","        if dev_mse < min_mse:\n","            # Save model if your model improved\n","            min_mse = dev_mse\n","            print('Saving model (epoch = {:4d}, loss = {:.4f})'\n","                .format(epoch + 1, min_mse))\n","            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n","            early_stop_cnt = 0\n","        else:\n","            early_stop_cnt += 1\n","\n","        loss_record['dev'].append(dev_mse)\n","        if early_stop_cnt > config['early_stop']:\n","            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n","            break\n","    \n","    print('Finished training after {} epochs'.format(epoch))\n","    return min_mse, loss_record\n","\n","\n","# fuction to compute mse in validation\n","def dev(dv_set, model, device):\n","    model.eval()  # set model to evalutation mode\n","    total_loss = 0\n","    for x, y in dv_set:  # iterate through the dataloader\n","        x, y = x.to(device), y.to(device)  # move data to device (cpu/cuda)\n","        with torch.no_grad():  # disable gradient calculation\n","            pred = model(x)  # forward pass (compute output)\n","            mse_loss = model.cal_loss(pred, y)  # compute loss\n","        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n","    total_loss = total_loss / len(dv_set.dataset)  # compute averaged loss\n","\n","    return total_loss\n","\n","\n","# function for model test\n","def test(tt_set, model, device):\n","    model.eval()  # set model to evalutation mode\n","    preds = []\n","\n","    for data in tt_set:\n","        data = data.to(device)\n","        # we don't need compute gradient when testing\n","        with torch.no_grad():\n","            pred = model(data)\n","            preds.append(pred.detach().cpu())\n","        preds = torch.cat(preds, dim=0).numpy()\n","    return preds\n","\n","\n","\n","# load data\n","data = np.load('timit_11/train_11.npy')  # load training data\n","print('size of dataset from train_11.npy: {}'.format(data.shape))  # (1229932, 429)\n","\n","label = np.load('timit_11/train_label_11.npy')  # load labels\n","print('size of dataset from train_label_11.npy: {}'.format(label.shape))  # (1229932,)\n","\n","device = get_device()\n","print('your device: {}'.format(device))\n","\n","# parameters for training\n","config = {\n","    'num_epochs': 10,\n","    'batch_size': 270,\n","    'optimizer': 'SGD',  # optimizer algorithm\n","    'optim_hparas': {\n","        'lr': 0.001,  # learning rate\n","        'momentum': 0.9  # momentum for SGD\n","    },\n","    'save_path': 'models/model.pth',\n","    'early_stop': 200\n","}\n","\n","# split data for training and validation\n","train_data, train_label, val_data, val_label = split_data(data=data, label=label)\n","\n","train_set = VoiceDataset(train_data, train_label)  # dataset for training\n","train_loader = DataLoader(dataset=train_set, batch_size=config['batch_size'], shuffle=True)\n","\n","val_set = VoiceDataset(val_data, val_label)\n","val_loader = DataLoader(dataset=val_set, batch_size=config['batch_size'], shuffle=False)\n","\n","# delete the data loaded to save space\n","del data, label, train_data, train_label, val_data, val_label\n","gc.collect()\n","\n","model = NeuralNet(429, 39).to(device)  # create network\n","\n","min_mse, loss_record = train(train_loader, val_loader, model, config, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPG2LqeHXmu0","executionInfo":{"status":"ok","timestamp":1678452948751,"user_tz":-480,"elapsed":174569,"user":{"displayName":"helfenstein xiao","userId":"14494537840267835334"}},"outputId":"39ba1edf-e86f-4e86-c7f7-ec7f7feb9d53"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["size of dataset from train_11.npy: (1229932, 429)\n","size of dataset from train_label_11.npy: (1229932,)\n","your device: cuda\n","Size of training set: (983945, 429)\n","Size of validation set: (245987, 429)\n","Epoch [1/10], Step [100/3645], Loss: 3.3730\n","Epoch [1/10], Step [200/3645], Loss: 3.3175\n","Epoch [1/10], Step [300/3645], Loss: 3.2632\n","Epoch [1/10], Step [400/3645], Loss: 3.2415\n","Epoch [1/10], Step [500/3645], Loss: 3.2773\n","Epoch [1/10], Step [600/3645], Loss: 3.2811\n","Epoch [1/10], Step [700/3645], Loss: 3.3957\n","Epoch [1/10], Step [800/3645], Loss: 3.2213\n","Epoch [1/10], Step [900/3645], Loss: 3.2584\n","Epoch [1/10], Step [1000/3645], Loss: 3.3027\n","Epoch [1/10], Step [1100/3645], Loss: 3.3683\n","Epoch [1/10], Step [1200/3645], Loss: 3.2978\n","Epoch [1/10], Step [1300/3645], Loss: 3.2756\n","Epoch [1/10], Step [1400/3645], Loss: 3.2509\n","Epoch [1/10], Step [1500/3645], Loss: 3.2471\n","Epoch [1/10], Step [1600/3645], Loss: 3.1940\n","Epoch [1/10], Step [1700/3645], Loss: 3.2667\n","Epoch [1/10], Step [1800/3645], Loss: 3.3257\n","Epoch [1/10], Step [1900/3645], Loss: 3.2291\n","Epoch [1/10], Step [2000/3645], Loss: 3.2434\n","Epoch [1/10], Step [2100/3645], Loss: 3.4163\n","Epoch [1/10], Step [2200/3645], Loss: 3.2506\n","Epoch [1/10], Step [2300/3645], Loss: 3.2811\n","Epoch [1/10], Step [2400/3645], Loss: 3.3775\n","Epoch [1/10], Step [2500/3645], Loss: 3.2605\n","Epoch [1/10], Step [2600/3645], Loss: 3.2558\n","Epoch [1/10], Step [2700/3645], Loss: 3.2596\n","Epoch [1/10], Step [2800/3645], Loss: 3.1874\n","Epoch [1/10], Step [2900/3645], Loss: 3.3430\n","Epoch [1/10], Step [3000/3645], Loss: 3.2933\n","Epoch [1/10], Step [3100/3645], Loss: 3.1648\n","Epoch [1/10], Step [3200/3645], Loss: 3.2540\n","Epoch [1/10], Step [3300/3645], Loss: 3.2759\n","Epoch [1/10], Step [3400/3645], Loss: 3.3219\n","Epoch [1/10], Step [3500/3645], Loss: 3.1770\n","Epoch [1/10], Step [3600/3645], Loss: 3.3084\n","Saving model (epoch =    1, loss = 3.2776)\n","Epoch [2/10], Step [100/3645], Loss: 3.2509\n","Epoch [2/10], Step [200/3645], Loss: 3.2352\n","Epoch [2/10], Step [300/3645], Loss: 3.2834\n","Epoch [2/10], Step [400/3645], Loss: 3.2398\n","Epoch [2/10], Step [500/3645], Loss: 3.3012\n","Epoch [2/10], Step [600/3645], Loss: 3.3757\n","Epoch [2/10], Step [700/3645], Loss: 3.2942\n","Epoch [2/10], Step [800/3645], Loss: 3.2123\n","Epoch [2/10], Step [900/3645], Loss: 3.1866\n","Epoch [2/10], Step [1000/3645], Loss: 3.2302\n","Epoch [2/10], Step [1100/3645], Loss: 3.2436\n","Epoch [2/10], Step [1200/3645], Loss: 3.2340\n","Epoch [2/10], Step [1300/3645], Loss: 3.2305\n","Epoch [2/10], Step [1400/3645], Loss: 3.2996\n","Epoch [2/10], Step [1500/3645], Loss: 3.2981\n","Epoch [2/10], Step [1600/3645], Loss: 3.2416\n","Epoch [2/10], Step [1700/3645], Loss: 3.2633\n","Epoch [2/10], Step [1800/3645], Loss: 3.1988\n","Epoch [2/10], Step [1900/3645], Loss: 3.2082\n","Epoch [2/10], Step [2000/3645], Loss: 3.2065\n","Epoch [2/10], Step [2100/3645], Loss: 3.2920\n","Epoch [2/10], Step [2200/3645], Loss: 3.2701\n","Epoch [2/10], Step [2300/3645], Loss: 3.3481\n","Epoch [2/10], Step [2400/3645], Loss: 3.2046\n","Epoch [2/10], Step [2500/3645], Loss: 3.2368\n","Epoch [2/10], Step [2600/3645], Loss: 3.2813\n","Epoch [2/10], Step [2700/3645], Loss: 3.2550\n","Epoch [2/10], Step [2800/3645], Loss: 3.2321\n","Epoch [2/10], Step [2900/3645], Loss: 3.2115\n","Epoch [2/10], Step [3000/3645], Loss: 3.1386\n","Epoch [2/10], Step [3100/3645], Loss: 3.3231\n","Epoch [2/10], Step [3200/3645], Loss: 3.2998\n","Epoch [2/10], Step [3300/3645], Loss: 3.2027\n","Epoch [2/10], Step [3400/3645], Loss: 3.2998\n","Epoch [2/10], Step [3500/3645], Loss: 3.2959\n","Epoch [2/10], Step [3600/3645], Loss: 3.2527\n","Saving model (epoch =    2, loss = 3.2562)\n","Epoch [3/10], Step [100/3645], Loss: 3.2621\n","Epoch [3/10], Step [200/3645], Loss: 3.2401\n","Epoch [3/10], Step [300/3645], Loss: 3.2399\n","Epoch [3/10], Step [400/3645], Loss: 3.3428\n","Epoch [3/10], Step [500/3645], Loss: 3.2797\n","Epoch [3/10], Step [600/3645], Loss: 3.2558\n","Epoch [3/10], Step [700/3645], Loss: 3.2647\n","Epoch [3/10], Step [800/3645], Loss: 3.2153\n","Epoch [3/10], Step [900/3645], Loss: 3.1708\n","Epoch [3/10], Step [1000/3645], Loss: 3.2088\n","Epoch [3/10], Step [1100/3645], Loss: 3.2906\n","Epoch [3/10], Step [1200/3645], Loss: 3.1405\n","Epoch [3/10], Step [1300/3645], Loss: 3.2944\n","Epoch [3/10], Step [1400/3645], Loss: 3.2126\n","Epoch [3/10], Step [1500/3645], Loss: 3.0462\n","Epoch [3/10], Step [1600/3645], Loss: 3.1204\n","Epoch [3/10], Step [1700/3645], Loss: 3.2567\n","Epoch [3/10], Step [1800/3645], Loss: 3.0908\n","Epoch [3/10], Step [1900/3645], Loss: 3.2150\n","Epoch [3/10], Step [2000/3645], Loss: 3.1367\n","Epoch [3/10], Step [2100/3645], Loss: 3.0516\n","Epoch [3/10], Step [2200/3645], Loss: 3.0801\n","Epoch [3/10], Step [2300/3645], Loss: 3.1136\n","Epoch [3/10], Step [2400/3645], Loss: 3.0764\n","Epoch [3/10], Step [2500/3645], Loss: 3.1367\n","Epoch [3/10], Step [2600/3645], Loss: 3.0131\n","Epoch [3/10], Step [2700/3645], Loss: 3.0934\n","Epoch [3/10], Step [2800/3645], Loss: 2.9457\n","Epoch [3/10], Step [2900/3645], Loss: 3.0002\n","Epoch [3/10], Step [3000/3645], Loss: 2.9383\n","Epoch [3/10], Step [3100/3645], Loss: 3.0582\n","Epoch [3/10], Step [3200/3645], Loss: 2.8815\n","Epoch [3/10], Step [3300/3645], Loss: 2.9658\n","Epoch [3/10], Step [3400/3645], Loss: 3.0062\n","Epoch [3/10], Step [3500/3645], Loss: 3.0350\n","Epoch [3/10], Step [3600/3645], Loss: 2.8756\n","Saving model (epoch =    3, loss = 2.9074)\n","Epoch [4/10], Step [100/3645], Loss: 2.8221\n","Epoch [4/10], Step [200/3645], Loss: 2.9281\n","Epoch [4/10], Step [300/3645], Loss: 2.9874\n","Epoch [4/10], Step [400/3645], Loss: 2.8907\n","Epoch [4/10], Step [500/3645], Loss: 2.8567\n","Epoch [4/10], Step [600/3645], Loss: 2.9071\n","Epoch [4/10], Step [700/3645], Loss: 2.8625\n","Epoch [4/10], Step [800/3645], Loss: 2.8837\n","Epoch [4/10], Step [900/3645], Loss: 2.7943\n","Epoch [4/10], Step [1000/3645], Loss: 2.8363\n","Epoch [4/10], Step [1100/3645], Loss: 2.7897\n","Epoch [4/10], Step [1200/3645], Loss: 2.9720\n","Epoch [4/10], Step [1300/3645], Loss: 2.7862\n","Epoch [4/10], Step [1400/3645], Loss: 2.8144\n","Epoch [4/10], Step [1500/3645], Loss: 2.6879\n","Epoch [4/10], Step [1600/3645], Loss: 2.8306\n","Epoch [4/10], Step [1700/3645], Loss: 2.7610\n","Epoch [4/10], Step [1800/3645], Loss: 2.7016\n","Epoch [4/10], Step [1900/3645], Loss: 2.8413\n","Epoch [4/10], Step [2000/3645], Loss: 2.8796\n","Epoch [4/10], Step [2100/3645], Loss: 2.7774\n","Epoch [4/10], Step [2200/3645], Loss: 2.7169\n","Epoch [4/10], Step [2300/3645], Loss: 2.7612\n","Epoch [4/10], Step [2400/3645], Loss: 2.7212\n","Epoch [4/10], Step [2500/3645], Loss: 2.6313\n","Epoch [4/10], Step [2600/3645], Loss: 2.8366\n","Epoch [4/10], Step [2700/3645], Loss: 2.6062\n","Epoch [4/10], Step [2800/3645], Loss: 2.6114\n","Epoch [4/10], Step [2900/3645], Loss: 2.7037\n","Epoch [4/10], Step [3000/3645], Loss: 2.7493\n","Epoch [4/10], Step [3100/3645], Loss: 2.8544\n","Epoch [4/10], Step [3200/3645], Loss: 2.7849\n","Epoch [4/10], Step [3300/3645], Loss: 2.8478\n","Epoch [4/10], Step [3400/3645], Loss: 2.8679\n","Epoch [4/10], Step [3500/3645], Loss: 2.8102\n","Epoch [4/10], Step [3600/3645], Loss: 2.8145\n","Saving model (epoch =    4, loss = 2.7504)\n","Epoch [5/10], Step [100/3645], Loss: 2.8584\n","Epoch [5/10], Step [200/3645], Loss: 2.7898\n","Epoch [5/10], Step [300/3645], Loss: 2.7211\n","Epoch [5/10], Step [400/3645], Loss: 2.7553\n","Epoch [5/10], Step [500/3645], Loss: 2.7313\n","Epoch [5/10], Step [600/3645], Loss: 2.6908\n","Epoch [5/10], Step [700/3645], Loss: 2.8025\n","Epoch [5/10], Step [800/3645], Loss: 2.7713\n","Epoch [5/10], Step [900/3645], Loss: 2.7996\n","Epoch [5/10], Step [1000/3645], Loss: 2.7205\n","Epoch [5/10], Step [1100/3645], Loss: 2.6758\n","Epoch [5/10], Step [1200/3645], Loss: 2.6599\n","Epoch [5/10], Step [1300/3645], Loss: 2.6967\n","Epoch [5/10], Step [1400/3645], Loss: 2.7833\n","Epoch [5/10], Step [1500/3645], Loss: 2.7470\n","Epoch [5/10], Step [1600/3645], Loss: 2.8305\n","Epoch [5/10], Step [1700/3645], Loss: 2.6787\n","Epoch [5/10], Step [1800/3645], Loss: 2.7416\n","Epoch [5/10], Step [1900/3645], Loss: 2.7102\n","Epoch [5/10], Step [2000/3645], Loss: 2.5881\n","Epoch [5/10], Step [2100/3645], Loss: 2.6745\n","Epoch [5/10], Step [2200/3645], Loss: 2.7417\n","Epoch [5/10], Step [2300/3645], Loss: 2.7496\n","Epoch [5/10], Step [2400/3645], Loss: 2.7444\n","Epoch [5/10], Step [2500/3645], Loss: 2.8185\n","Epoch [5/10], Step [2600/3645], Loss: 2.7213\n","Epoch [5/10], Step [2700/3645], Loss: 2.7552\n","Epoch [5/10], Step [2800/3645], Loss: 2.5867\n","Epoch [5/10], Step [2900/3645], Loss: 2.7489\n","Epoch [5/10], Step [3000/3645], Loss: 2.6821\n","Epoch [5/10], Step [3100/3645], Loss: 2.6288\n","Epoch [5/10], Step [3200/3645], Loss: 2.6757\n","Epoch [5/10], Step [3300/3645], Loss: 2.7331\n","Epoch [5/10], Step [3400/3645], Loss: 2.7226\n","Epoch [5/10], Step [3500/3645], Loss: 2.6743\n","Epoch [5/10], Step [3600/3645], Loss: 2.6821\n","Saving model (epoch =    5, loss = 2.6845)\n","Epoch [6/10], Step [100/3645], Loss: 2.6713\n","Epoch [6/10], Step [200/3645], Loss: 2.6779\n","Epoch [6/10], Step [300/3645], Loss: 2.6779\n","Epoch [6/10], Step [400/3645], Loss: 2.7014\n","Epoch [6/10], Step [500/3645], Loss: 2.5336\n","Epoch [6/10], Step [600/3645], Loss: 2.7439\n","Epoch [6/10], Step [700/3645], Loss: 2.6586\n","Epoch [6/10], Step [800/3645], Loss: 2.5816\n","Epoch [6/10], Step [900/3645], Loss: 2.6713\n","Epoch [6/10], Step [1000/3645], Loss: 2.5528\n","Epoch [6/10], Step [1100/3645], Loss: 2.5922\n","Epoch [6/10], Step [1200/3645], Loss: 2.6532\n","Epoch [6/10], Step [1300/3645], Loss: 2.6714\n","Epoch [6/10], Step [1400/3645], Loss: 2.6937\n","Epoch [6/10], Step [1500/3645], Loss: 2.6255\n","Epoch [6/10], Step [1600/3645], Loss: 2.6735\n","Epoch [6/10], Step [1700/3645], Loss: 2.6871\n","Epoch [6/10], Step [1800/3645], Loss: 2.6360\n","Epoch [6/10], Step [1900/3645], Loss: 2.7184\n","Epoch [6/10], Step [2000/3645], Loss: 2.6320\n","Epoch [6/10], Step [2100/3645], Loss: 2.5383\n","Epoch [6/10], Step [2200/3645], Loss: 2.6092\n","Epoch [6/10], Step [2300/3645], Loss: 2.6290\n","Epoch [6/10], Step [2400/3645], Loss: 2.5810\n","Epoch [6/10], Step [2500/3645], Loss: 2.5954\n","Epoch [6/10], Step [2600/3645], Loss: 2.6471\n","Epoch [6/10], Step [2700/3645], Loss: 2.6723\n","Epoch [6/10], Step [2800/3645], Loss: 2.7145\n","Epoch [6/10], Step [2900/3645], Loss: 2.6342\n","Epoch [6/10], Step [3000/3645], Loss: 2.6837\n","Epoch [6/10], Step [3100/3645], Loss: 2.5659\n","Epoch [6/10], Step [3200/3645], Loss: 2.6805\n","Epoch [6/10], Step [3300/3645], Loss: 2.6660\n","Epoch [6/10], Step [3400/3645], Loss: 2.5124\n","Epoch [6/10], Step [3500/3645], Loss: 2.6097\n","Epoch [6/10], Step [3600/3645], Loss: 2.5856\n","Saving model (epoch =    6, loss = 2.6057)\n","Epoch [7/10], Step [100/3645], Loss: 2.7015\n","Epoch [7/10], Step [200/3645], Loss: 2.6615\n","Epoch [7/10], Step [300/3645], Loss: 2.6510\n","Epoch [7/10], Step [400/3645], Loss: 2.6459\n","Epoch [7/10], Step [500/3645], Loss: 2.6967\n","Epoch [7/10], Step [600/3645], Loss: 2.5106\n","Epoch [7/10], Step [700/3645], Loss: 2.6397\n","Epoch [7/10], Step [800/3645], Loss: 2.4880\n","Epoch [7/10], Step [900/3645], Loss: 2.5091\n","Epoch [7/10], Step [1000/3645], Loss: 2.6235\n","Epoch [7/10], Step [1100/3645], Loss: 2.5802\n","Epoch [7/10], Step [1200/3645], Loss: 2.5763\n","Epoch [7/10], Step [1300/3645], Loss: 2.5600\n","Epoch [7/10], Step [1400/3645], Loss: 2.5889\n","Epoch [7/10], Step [1500/3645], Loss: 2.6470\n","Epoch [7/10], Step [1600/3645], Loss: 2.5953\n","Epoch [7/10], Step [1700/3645], Loss: 2.5647\n","Epoch [7/10], Step [1800/3645], Loss: 2.6256\n","Epoch [7/10], Step [1900/3645], Loss: 2.5617\n","Epoch [7/10], Step [2000/3645], Loss: 2.5699\n","Epoch [7/10], Step [2100/3645], Loss: 2.4809\n","Epoch [7/10], Step [2200/3645], Loss: 2.5618\n","Epoch [7/10], Step [2300/3645], Loss: 2.4772\n","Epoch [7/10], Step [2400/3645], Loss: 2.5913\n","Epoch [7/10], Step [2500/3645], Loss: 2.5381\n","Epoch [7/10], Step [2600/3645], Loss: 2.5490\n","Epoch [7/10], Step [2700/3645], Loss: 2.5077\n","Epoch [7/10], Step [2800/3645], Loss: 2.4111\n","Epoch [7/10], Step [2900/3645], Loss: 2.3889\n","Epoch [7/10], Step [3000/3645], Loss: 2.4860\n","Epoch [7/10], Step [3100/3645], Loss: 2.3801\n","Epoch [7/10], Step [3200/3645], Loss: 2.4840\n","Epoch [7/10], Step [3300/3645], Loss: 2.4199\n","Epoch [7/10], Step [3400/3645], Loss: 2.5303\n","Epoch [7/10], Step [3500/3645], Loss: 2.4597\n","Epoch [7/10], Step [3600/3645], Loss: 2.4870\n","Saving model (epoch =    7, loss = 2.4648)\n","Epoch [8/10], Step [100/3645], Loss: 2.4227\n","Epoch [8/10], Step [200/3645], Loss: 2.3703\n","Epoch [8/10], Step [300/3645], Loss: 2.4937\n","Epoch [8/10], Step [400/3645], Loss: 2.5933\n","Epoch [8/10], Step [500/3645], Loss: 2.4950\n","Epoch [8/10], Step [600/3645], Loss: 2.4037\n","Epoch [8/10], Step [700/3645], Loss: 2.3918\n","Epoch [8/10], Step [800/3645], Loss: 2.5253\n","Epoch [8/10], Step [900/3645], Loss: 2.4238\n","Epoch [8/10], Step [1000/3645], Loss: 2.4070\n","Epoch [8/10], Step [1100/3645], Loss: 2.4960\n","Epoch [8/10], Step [1200/3645], Loss: 2.4840\n","Epoch [8/10], Step [1300/3645], Loss: 2.3584\n","Epoch [8/10], Step [1400/3645], Loss: 2.3841\n","Epoch [8/10], Step [1500/3645], Loss: 2.2925\n","Epoch [8/10], Step [1600/3645], Loss: 2.3846\n","Epoch [8/10], Step [1700/3645], Loss: 2.4350\n","Epoch [8/10], Step [1800/3645], Loss: 2.3542\n","Epoch [8/10], Step [1900/3645], Loss: 2.3553\n","Epoch [8/10], Step [2000/3645], Loss: 2.4574\n","Epoch [8/10], Step [2100/3645], Loss: 2.4272\n","Epoch [8/10], Step [2200/3645], Loss: 2.5058\n","Epoch [8/10], Step [2300/3645], Loss: 2.5041\n","Epoch [8/10], Step [2400/3645], Loss: 2.4555\n","Epoch [8/10], Step [2500/3645], Loss: 2.4306\n","Epoch [8/10], Step [2600/3645], Loss: 2.4167\n","Epoch [8/10], Step [2700/3645], Loss: 2.4125\n","Epoch [8/10], Step [2800/3645], Loss: 2.3257\n","Epoch [8/10], Step [2900/3645], Loss: 2.3717\n","Epoch [8/10], Step [3000/3645], Loss: 2.3291\n","Epoch [8/10], Step [3100/3645], Loss: 2.3630\n","Epoch [8/10], Step [3200/3645], Loss: 2.4507\n","Epoch [8/10], Step [3300/3645], Loss: 2.3926\n","Epoch [8/10], Step [3400/3645], Loss: 2.2354\n","Epoch [8/10], Step [3500/3645], Loss: 2.4046\n","Epoch [8/10], Step [3600/3645], Loss: 2.1976\n","Saving model (epoch =    8, loss = 2.3364)\n","Epoch [9/10], Step [100/3645], Loss: 2.3626\n","Epoch [9/10], Step [200/3645], Loss: 2.2351\n","Epoch [9/10], Step [300/3645], Loss: 2.2258\n","Epoch [9/10], Step [400/3645], Loss: 2.3952\n","Epoch [9/10], Step [500/3645], Loss: 2.3633\n","Epoch [9/10], Step [600/3645], Loss: 2.3839\n","Epoch [9/10], Step [700/3645], Loss: 2.2456\n","Epoch [9/10], Step [800/3645], Loss: 2.3845\n","Epoch [9/10], Step [900/3645], Loss: 2.2963\n","Epoch [9/10], Step [1000/3645], Loss: 2.2830\n","Epoch [9/10], Step [1100/3645], Loss: 2.3473\n","Epoch [9/10], Step [1200/3645], Loss: 2.3767\n","Epoch [9/10], Step [1300/3645], Loss: 2.3140\n","Epoch [9/10], Step [1400/3645], Loss: 2.3538\n","Epoch [9/10], Step [1500/3645], Loss: 2.1572\n","Epoch [9/10], Step [1600/3645], Loss: 2.2032\n","Epoch [9/10], Step [1700/3645], Loss: 2.3007\n","Epoch [9/10], Step [1800/3645], Loss: 2.3027\n","Epoch [9/10], Step [1900/3645], Loss: 2.3481\n","Epoch [9/10], Step [2000/3645], Loss: 2.3668\n","Epoch [9/10], Step [2100/3645], Loss: 2.3438\n","Epoch [9/10], Step [2200/3645], Loss: 2.2617\n","Epoch [9/10], Step [2300/3645], Loss: 2.3204\n","Epoch [9/10], Step [2400/3645], Loss: 2.3358\n","Epoch [9/10], Step [2500/3645], Loss: 2.3217\n","Epoch [9/10], Step [2600/3645], Loss: 2.2237\n","Epoch [9/10], Step [2700/3645], Loss: 2.3052\n","Epoch [9/10], Step [2800/3645], Loss: 2.3249\n","Epoch [9/10], Step [2900/3645], Loss: 2.2499\n","Epoch [9/10], Step [3000/3645], Loss: 2.2858\n","Epoch [9/10], Step [3100/3645], Loss: 2.1435\n","Epoch [9/10], Step [3200/3645], Loss: 2.1560\n","Epoch [9/10], Step [3300/3645], Loss: 2.1974\n","Epoch [9/10], Step [3400/3645], Loss: 2.2067\n","Epoch [9/10], Step [3500/3645], Loss: 2.0828\n","Epoch [9/10], Step [3600/3645], Loss: 2.2901\n","Saving model (epoch =    9, loss = 2.2485)\n","Epoch [10/10], Step [100/3645], Loss: 2.3603\n","Epoch [10/10], Step [200/3645], Loss: 2.1937\n","Epoch [10/10], Step [300/3645], Loss: 2.3469\n","Epoch [10/10], Step [400/3645], Loss: 2.3910\n","Epoch [10/10], Step [500/3645], Loss: 2.1906\n","Epoch [10/10], Step [600/3645], Loss: 2.3509\n","Epoch [10/10], Step [700/3645], Loss: 2.2731\n","Epoch [10/10], Step [800/3645], Loss: 2.1294\n","Epoch [10/10], Step [900/3645], Loss: 2.1953\n","Epoch [10/10], Step [1000/3645], Loss: 2.3061\n","Epoch [10/10], Step [1100/3645], Loss: 2.3844\n","Epoch [10/10], Step [1200/3645], Loss: 2.1998\n","Epoch [10/10], Step [1300/3645], Loss: 2.2676\n","Epoch [10/10], Step [1400/3645], Loss: 2.2887\n","Epoch [10/10], Step [1500/3645], Loss: 2.1866\n","Epoch [10/10], Step [1600/3645], Loss: 2.2745\n","Epoch [10/10], Step [1700/3645], Loss: 2.2142\n","Epoch [10/10], Step [1800/3645], Loss: 2.0787\n","Epoch [10/10], Step [1900/3645], Loss: 2.1245\n","Epoch [10/10], Step [2000/3645], Loss: 2.2545\n","Epoch [10/10], Step [2100/3645], Loss: 2.2235\n","Epoch [10/10], Step [2200/3645], Loss: 2.1650\n","Epoch [10/10], Step [2300/3645], Loss: 2.1499\n","Epoch [10/10], Step [2400/3645], Loss: 2.2186\n","Epoch [10/10], Step [2500/3645], Loss: 2.2556\n","Epoch [10/10], Step [2600/3645], Loss: 2.1440\n","Epoch [10/10], Step [2700/3645], Loss: 2.2908\n","Epoch [10/10], Step [2800/3645], Loss: 2.2234\n","Epoch [10/10], Step [2900/3645], Loss: 2.3117\n","Epoch [10/10], Step [3000/3645], Loss: 2.1966\n","Epoch [10/10], Step [3100/3645], Loss: 2.1021\n","Epoch [10/10], Step [3200/3645], Loss: 2.0490\n","Epoch [10/10], Step [3300/3645], Loss: 2.3849\n","Epoch [10/10], Step [3400/3645], Loss: 2.2506\n","Epoch [10/10], Step [3500/3645], Loss: 2.2969\n","Epoch [10/10], Step [3600/3645], Loss: 2.2371\n","Saving model (epoch =   10, loss = 2.1739)\n","Finished training after 9 epochs\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","\n","def plot_learning_curve(loss_record, title=''):\n","    ''' Plot learning curve of your DNN (train & dev loss) '''\n","    total_steps = len(loss_record['train'])\n","    x_1 = range(total_steps)\n","    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n","    figure(figsize=(6, 4))\n","    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n","    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n","    plt.ylim(0.0, 5.)\n","    plt.xlabel('Training steps')\n","    plt.ylabel('MSE loss')\n","    plt.title('Learning curve of {}'.format(title))\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"DRZ_0wLCnDGl","executionInfo":{"status":"ok","timestamp":1678453298528,"user_tz":-480,"elapsed":699,"user":{"displayName":"helfenstein xiao","userId":"14494537840267835334"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(min_mse)\n","plot_learning_curve(loss_record, title='deep model')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"XE_mjW7WmRVK","executionInfo":{"status":"ok","timestamp":1678453345573,"user_tz":-480,"elapsed":1239,"user":{"displayName":"helfenstein xiao","userId":"14494537840267835334"}},"outputId":"cd8a77f9-6462-41a9-9956-954e597c0f6b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2.1738587954396493\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}